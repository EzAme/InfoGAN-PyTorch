{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import random\n",
    "\n",
    "from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "from dataloader import get_data\n",
    "from utils import *\n",
    "from config import params\n",
    "\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'HGD'):\n",
    "    from models.hgd_model3 import Generator, Discriminator, DHead, QHead, AE, LogitHead, VAEncoder\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    from models.svhn_model import Generator, Discriminator, DHead, QHead, AE\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    from models.celeba_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1123\n",
      "cuda:0  will be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility.\n",
    "seed = 1123\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "# Use GPU if available.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dataloader = get_data(params['dataset'], params['batch_size'])\n",
    "\n",
    "# Set appropriate hyperparameters depending on the dataset used.\n",
    "# The values given in the InfoGAN paper are used.\n",
    "# num_z : dimension of incompressible noise.\n",
    "# num_dis_c : number of discrete latent code used.\n",
    "# dis_c_dim : dimension of discrete latent code.\n",
    "# num_con_c : number of continuous latent code used.\n",
    "if (params['dataset'] == 'HGD'):\n",
    "    params['num_z'] = 256\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 4\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    params['num_z'] = 124\n",
    "    params['num_dis_c'] = 4\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 4\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    params['num_z'] = 128\n",
    "    params['num_dis_c'] = 10\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 0\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "\n",
    "# Plot the training images.\n",
    "\n",
    "# sample_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(\n",
    "#     sample_batch[0].to(device)[ : 100], nrow=10, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "# plt.savefig('results\\Training Images {}'.format(params['dataset']))\n",
    "# plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (noise_latent): Linear(in_features=256, out_features=16384, bias=False)\n",
      "  (label): Linear(in_features=1, out_features=64, bias=False)\n",
      "  (Elabel): Embedding(10, 1)\n",
      "  (cont_latent): Linear(in_features=4, out_features=256, bias=False)\n",
      "  (tconv1): ConvTranspose2d(261, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(261, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv2): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv3): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (label): Linear(in_features=1, out_features=4096, bias=True)\n",
      "  (Elabel): Embedding(10, 1)\n",
      "  (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 142, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(142, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "DHead(\n",
      "  (conv): Conv2d(142, 1, kernel_size=(8, 8), stride=(1, 1))\n",
      ")\n",
      "QHead(\n",
      "  (conv1): Conv2d(142, 128, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_disc): Conv2d(128, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_mu): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "VAEncoder(\n",
      "  (label): Linear(in_features=1, out_features=64, bias=False)\n",
      "  (Elabel): Embedding(10, 1)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mu): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the network.\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "print(discriminator)\n",
    "discriminator.register_module_forward_hook()\n",
    "netD = DHead().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "netQ = QHead().to(device)\n",
    "netQ.apply(weights_init)\n",
    "print(netQ)\n",
    "\n",
    "vae = VAEncoder().to(device)\n",
    "vae.apply(weights_init)\n",
    "print(vae)\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "\n",
    "# Loss for discrimination between real and fake images.\n",
    "criterionD = nn.BCELoss()\n",
    "# Loss for discrete latent code.\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "# Loss for continuous latent code.\n",
    "criterionQ_con = NormalNLLLoss()\n",
    "\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "# Loss for image\n",
    "criterionRECON = nn.L1Loss()\n",
    "# Loss for latent encoding\n",
    "criterionLAT = nn.L1Loss()\n",
    "\n",
    "# Adam optimiser is used.\n",
    "optimD = optim.Adam([{'params': discriminator.parameters()}, {'params': netD.parameters()}], lr=params['learning_rate'], betas=(params['beta1'], params['beta2']))\n",
    "optimG = optim.Adam([{'params': netG.parameters()}, {'params': netQ.parameters()}], lr=params['learning_rate_g'], betas=(.4, params['beta2']))\n",
    "# schedulerD = optim.ReduceLROnPlateau(optimD, 'min')\n",
    "# schedulerG = optim.ReduceLROnPlateau(optimG, 'min')\n",
    "# Fixed Noise\n",
    "z = torch.randn(100, params['num_z'], device=device)\n",
    "fixed_noise = z\n",
    "if(params['num_dis_c'] != 0):\n",
    "    idx = np.arange(params['dis_c_dim']).repeat(10)\n",
    "    dis_c = torch.zeros(100, params['num_dis_c'], params['dis_c_dim'], device=device)\n",
    "    for i in range(params['num_dis_c']):\n",
    "        dis_c[torch.arange(0, 100), i, idx] = 1.0\n",
    "\n",
    "    dis_c = dis_c.view(100, -1)\n",
    "\n",
    "    # fixed_noise = torch.cat((fixed_noise, dis_c), dim=1)\n",
    "fixed_idx= torch.tensor(idx,device = device,dtype = torch.long).reshape(-1,1)\n",
    "if(params['num_con_c'] != 0):\n",
    "    con_c = torch.rand(100, params['num_con_c'],  device=device) * 2 - 1\n",
    "    # fixed_noise = torch.cat((fixed_noise, con_c), dim=1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0.0\n",
    "\n",
    "# List variables to store results pf training.\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "noise, idx, gt_fake, continuous = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], 100, device)\n",
    "fake_data = netG(fixed_noise,fixed_idx,con_c)\n",
    "noise = vae(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"Starting Training Loop...\\n\")\n",
    "print('Epochs: %d\\nDataset: {}\\nBatch Size: %d\\nLength of Data Loader: %d'.format(params['dataset']) % (params['num_epochs'], params['batch_size'], len(dataloader)))\n",
    "print(\"-\"*25)\n",
    "\n",
    "start_time = time.time()\n",
    "iters = 0\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (data, gt_labels) in enumerate(dataloader, 0):\n",
    "        # Get batch size\n",
    "        b_size = data.size(0)\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        real_data = data.to(device)\n",
    "        gt_labels = gt_labels.unsqueeze(1).to(device,dtype = torch.long)\n",
    "\n",
    "        # Updating discriminator and DHead\n",
    "        optimD.zero_grad()\n",
    "        # Real data\n",
    "        label = torch.full((b_size, ), real_label, device=device,dtype=torch.float32)\n",
    "        # print(real_data.shape,'realdata')\n",
    "        # print(real_data.shape)\n",
    "        output1 = discriminator(real_data, gt_labels)\n",
    "        # print(output1.shape,'output1')\n",
    "        # output_real_DG = netDG(output1)\n",
    "        # print(output_real_DG.shape,'dgshape')\n",
    "        # print(real_data.shape,output_real_DG.shape)\n",
    "        # loss_real_ae= criterion_mse(real_data,output_real_DG)\n",
    "        probs_real = netD(output1).view(-1)\n",
    "        # classifier,_,_ = netQ(output1)\n",
    "\n",
    "        # print(label.dtype,probs_real.dtype)\n",
    "        # print(classifier.shape, gt_labels.shape)\n",
    "        # lossR_class = criterionQ_dis(classifier,gt_labels.squeeze())\n",
    "        # print(classifier,gt_labels)\n",
    "        loss_real = criterionD(probs_real, label)\n",
    "        # Calculate gradients.\n",
    "        # loss_real+=loss_real_ae\n",
    "        lossR= loss_real# + lossR_class\n",
    "        lossR.backward()\n",
    "\n",
    "        # Fake data\n",
    "        label.fill_(fake_label)\n",
    "        _,idx, _, continuous = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], b_size, device)\n",
    "        z,real_mu,real_var = vae(real_data, gt_labels)\n",
    "        \n",
    "        target = torch.LongTensor(idx).to(device)\n",
    "        fake_data = netG(z,gt_labels,continuous)\n",
    "        # print(fake_data.shape,'fake_data1')\n",
    "        output2 = discriminator(fake_data.detach(),gt_labels)\n",
    "        # classifier_fake,_,_ = netQ(output2)\n",
    "        # print(classifier.shape,target_d.shape)\n",
    "        # output_fake_DG = netDG(output2)\n",
    "        # print(output_real_DG.shape,'dgshape')\n",
    "        # print(real_data.shape,output_real_DG.shape)\n",
    "        # loss_fake_ae= criterion_mse(fake_data.detach(),output_fake_DG)\n",
    "        # lossF_class = criterionQ_dis(classifier_fake,target.reshape(gt_labels.shape))\n",
    "\n",
    "\n",
    "        probs_fake = netD(output2).view(-1)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        # Calculate gradients.\n",
    "        # lossF =loss_fake + lossF_class# + loss_fake_ae\n",
    "        loss_fake.backward()\n",
    "       \n",
    "\n",
    "        # Net Loss for the discriminator\n",
    "        D_loss = loss_fake + loss_real\n",
    "        # Update parameters\n",
    "        optimD.step()\n",
    "\n",
    "        # Updating Generator and QHead\n",
    "        optimG.zero_grad()\n",
    "        \n",
    "        # Fake data treated as real.\n",
    "        # print(fake_data.shape,'fake data shpe')\n",
    "        output = discriminator(fake_data,gt_labels)\n",
    "        # print(output, 'output discrim')\n",
    "        label.fill_(real_label)\n",
    "        # fake_logit = netLogit(output)\n",
    "\n",
    "        probs_fake = netD(output).view(-1)\n",
    "        #output_fake_DG = netDG(output)\n",
    "        #loss_fake_ae= criterion_mse(fake_data,output_fake_DG)\n",
    "        gen_loss = criterionD(probs_fake, label)\n",
    "        q_logits, q_mu, q_var = netQ(output)\n",
    "        recon_loss = criterionRECON(real_data,fake_data)\n",
    "        \n",
    "        # gen_dis_loss = criterionQ_dis(fake_logit,q_logits)\n",
    "\n",
    "        # Calculating loss for discrete latent code.\n",
    "        # dis_loss = 0\n",
    "        dis_loss = criterionQ_dis(q_logits,target.to(dtype=torch.long).squeeze())\n",
    "        # for j in range(params['num_dis_c']):\n",
    "        #     dis_loss += criterionQ_dis(q_logits[:, j*10 : j*10 + 10], target[j])\n",
    "\n",
    "        # Calculating loss for continuous latent code.\n",
    "        con_loss = 0\n",
    "        if (params['num_con_c'] != 0):\n",
    "            con_loss = criterionQ_con(continuous, q_mu, q_var)\n",
    "        recon_loss = criterionRECON(real_data,fake_data)\n",
    "        latent_loss = criterionLAT((real_mu+ torch.randn_like(real_var)*torch.sqrt(real_var.exp())).view(b_size,-1,1,1),\n",
    "                                  (fake_mu+ torch.randn_like(fake_var)*torch.sqrt(fake_var.exp())).view(b_size,-1,1,1))\n",
    "        \n",
    "        KLD_loss  = torch.mean(-0.5 * torch.sum(1 + real_var - real_mu.pow(2) - real_var.exp(),dim=1)).to(device)\n",
    "        # Net loss for generator.\n",
    "        G_loss = gen_loss + dis_loss  + con_loss + recon_loss + latent_loss + KLD_loss #+  loss_fake_ae*.5+dis_loss\n",
    "        # print(G_loss)\n",
    "        # Calculate gradients.\n",
    "        G_loss.backward()\n",
    "        # Update parameters.\n",
    "        # torch.nn.utils.clip_grad_norm_(netG.parameters(), 1)\n",
    "        optimG.step()\n",
    "\n",
    "        # Check progress of training.\n",
    "        if i != 0 and i%100 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch+1, params['num_epochs'], i, len(dataloader), \n",
    "                   D_loss.item(), G_loss.item()))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        G_losses.append(G_loss.item())\n",
    "        D_losses.append(D_loss.item())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(\"Time taken for Epoch %d: %.2fs\" %(epoch + 1, epoch_time))\n",
    "    # Generate image after each epoch to check performance of the generator. Used for creating animated gif later.\n",
    "    with torch.no_grad():\n",
    "        gen_data = netG(z,fixed_idx,con_c).detach().cpu()\n",
    "    img_list.append(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True))\n",
    "\n",
    "    # Generate image to check performance of generator.\n",
    "    if((epoch+1) == 1 or (epoch+1) % 10==0):\n",
    "        with torch.no_grad():\n",
    "            gen_data = netG(z,fixed_idx,con_c).detach().cpu()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "        plt.savefig(\"results/Epoch_%d_{}\".format(params['dataset']) %(epoch+1))\n",
    "        plt.close('all')\n",
    "\n",
    "    # Save network weights.\n",
    "    if (epoch+1) % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'netG' : netG.state_dict(),\n",
    "            'discriminator' : discriminator.state_dict(),\n",
    "            'netD' : netD.state_dict(),\n",
    "            'netQ' : netQ.state_dict(),\n",
    "            'optimD' : optimD.state_dict(),\n",
    "            'optimG' : optimG.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'checkpoint/model_epoch_%d_{}'.format(params['dataset']) %(epoch+1))\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"-\"*50)\n",
    "print('Training finished!\\nTotal Time for Training: %.2fm' %(training_time / 60))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate image to check performance of trained generator.\n",
    "with torch.no_grad():\n",
    "    gen_data = netG(z,fixed_idx,con_c).detach().cpu()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.savefig(\"results/Epoch_%d_{}\".format(params['dataset']) %(params['num_epochs']))\n",
    "\n",
    "# Save network weights.\n",
    "torch.save({\n",
    "    'netG' : netG.state_dict(),\n",
    "    'discriminator' : discriminator.state_dict(),\n",
    "    'netD' : netD.state_dict(),\n",
    "    'netQ' : netQ.state_dict(),\n",
    "    'optimD' : optimD.state_dict(),\n",
    "    'optimG' : optimG.state_dict(),\n",
    "    'params' : params\n",
    "    }, 'checkpoint/model_final_{}'.format(params['dataset']))\n",
    "\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"results/Loss Curve {}\".format(params['dataset']))\n",
    "\n",
    "# Animation showing the improvements of the generator.\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "anim = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "anim.save('results/infoGAN_{}.gif'.format(params['dataset']), dpi=80, writer='imagemagick')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-load_path', required=True, help='Checkpoint to load path from')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "load_path = 'checkpoint/model_final_HGD2'\n",
    "\n",
    "from models.hgd_model2 import Generator\n",
    "\n",
    "# Load the checkpoint file\n",
    "state_dict = torch.load(load_path)\n",
    "\n",
    "# Set the device to run on: GPU or CPU.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "# Get the 'params' dictionary from the loaded state_dict.\n",
    "params = state_dict['params']\n",
    "\n",
    "# Create the generator network.\n",
    "netG = Generator().to(device)\n",
    "# Load the trained generator weights.\n",
    "netG.load_state_dict(state_dict['netG'])\n",
    "# print(netG)\n",
    "\n",
    "c = np.linspace(-2, 2, 10).reshape(1, -1)\n",
    "c = np.repeat(c, 10).reshape(-1, 1)\n",
    "c = torch.from_numpy(c).float().to(device)\n",
    "c = c.view(-1, 1)\n",
    "# print(c.shape,'cshape')\n",
    "zeros = torch.zeros(100, 1, device=device)\n",
    "# Continuous latent code.\n",
    "c2 = torch.cat((zeros, zeros,zeros,zeros), dim=1)\n",
    "c3 = torch.cat((zeros, c,zeros,zeros), dim=1)\n",
    "c4 = torch.cat((zeros,zeros,c, zeros), dim=1)\n",
    "c5 = torch.cat((zeros,zeros,zeros, c), dim=1)\n",
    "# print(c2)\n",
    "# print(c2.shape,'c2shape')\n",
    "idx = np.arange(10).repeat(10)\n",
    "dis_c = torch.zeros(100, 8, 10, 1, device=device)\n",
    "for i in range(8):\n",
    "    dis_c[torch.arange(0, 100),i, idx] = .9\n",
    "# Discrete latent code.\n",
    "c1 = dis_c.view(100, -1, 1, 1)\n",
    "# print(c1,'c1')\n",
    "z = torch.randn(100, 256, device=device)\n",
    "idx = torch.tensor(idx,device=device,dtype=torch.long).unsqueeze(1)\n",
    "# print(z.dtype,idx.dtype,c2.dtype)\n",
    "# To see variation along c2 (Horizontally) and c1 (Vertically)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img1 = netG(z,idx,c2).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img1, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img2 = netG(z,idx,c3).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img2, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_img1 = netG(z,idx,c4).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img1, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img2 = netG(z,idx,c5).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img2, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoGan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
