{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import random\n",
    "\n",
    "from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "from dataloader import get_data\n",
    "from utils import *\n",
    "from config import params\n",
    "\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'HGD'):\n",
    "    from models.hgd_model3 import Generator, Discriminator, DHead, QHead, AE, LogitHead, VAEncoder\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    from models.svhn_model import Generator, Discriminator, DHead, QHead, AE\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    from models.celeba_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1123\n",
      "cuda:0  will be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility.\n",
    "seed = 1123\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "# Use GPU if available.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dataloader = get_data(params['dataset'], params['batch_size'])\n",
    "\n",
    "# Set appropriate hyperparameters depending on the dataset used.\n",
    "# The values given in the InfoGAN paper are used.\n",
    "# num_z : dimension of incompressible noise.\n",
    "# num_dis_c : number of discrete latent code used.\n",
    "# dis_c_dim : dimension of discrete latent code.\n",
    "# num_con_c : number of continuous latent code used.\n",
    "if (params['dataset'] == 'HGD'):\n",
    "    params['num_z'] = 256\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 4\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    params['num_z'] = 124\n",
    "    params['num_dis_c'] = 4\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 4\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    params['num_z'] = 128\n",
    "    params['num_dis_c'] = 10\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 0\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "\n",
    "# Plot the training images.\n",
    "\n",
    "# sample_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(\n",
    "#     sample_batch[0].to(device)[ : 100], nrow=10, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "# plt.savefig('results\\Training Images {}'.format(params['dataset']))\n",
    "# plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (noise_latent): Linear(in_features=256, out_features=16384, bias=False)\n",
      "  (label): Linear(in_features=1, out_features=64, bias=False)\n",
      "  (Elabel): Embedding(10, 1)\n",
      "  (cont_latent): Linear(in_features=4, out_features=256, bias=False)\n",
      "  (tconv1): ConvTranspose2d(261, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(261, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv2): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv3): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (label): Linear(in_features=1, out_features=4096, bias=True)\n",
      "  (Elabel): Embedding(10, 1)\n",
      "  (conv1): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 142, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(142, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  (leaky_relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  (leaky_relu3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "DHead(\n",
      "  (conv): Conv2d(142, 1, kernel_size=(8, 8), stride=(1, 1))\n",
      ")\n",
      "QHead(\n",
      "  (conv1): Conv2d(142, 128, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_disc): Conv2d(128, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_mu): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "VAEncoder(\n",
      "  (label): Linear(in_features=1, out_features=4096, bias=True)\n",
      "  (Elabel): Embedding(10, 1)\n",
      "  (conv1): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(128, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mu): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the network.\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "print(discriminator)\n",
    "\n",
    "netD = DHead().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "netQ = QHead().to(device)\n",
    "netQ.apply(weights_init)\n",
    "print(netQ)\n",
    "\n",
    "vae = VAEncoder().to(device)\n",
    "vae.apply(weights_init)\n",
    "print(vae)\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model,input,output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "discriminator.leaky_relu1.register_forward_hook(get_activation('leaky_relu1'))\n",
    "discriminator.leaky_relu2.register_forward_hook(get_activation('leaky_relu2'))\n",
    "discriminator.leaky_relu3.register_forward_hook(get_activation('leaky_relu3'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss for discrimination between real and fake images.\n",
    "criterionD = nn.BCELoss()\n",
    "# Loss for discrete latent code.\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "# Loss for continuous latent code.\n",
    "criterionQ_con = NormalNLLLoss()\n",
    "\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "# Loss for image\n",
    "criterionRECON = nn.L1Loss()\n",
    "# Loss for latent encoding\n",
    "criterionLAT = nn.L1Loss()\n",
    "\n",
    "# Adam optimiser is used.\n",
    "optimD = optim.Adam([{'params': discriminator.parameters()}, {'params': netD.parameters()}], lr=params['learning_rate'], betas=(params['beta1'], params['beta2']))\n",
    "optimG = optim.Adam([{'params': vae.parameters()},{'params': netG.parameters()}, {'params': netQ.parameters()}], lr=params['learning_rate_g'], betas=(.4, params['beta2']))\n",
    "# schedulerD = optim.ReduceLROnPlateau(optimD, 'min')\n",
    "# schedulerG = optim.ReduceLROnPlateau(optimG, 'min')\n",
    "# Fixed Noise\n",
    "z = torch.randn(100, params['num_z'], device=device)\n",
    "fixed_noise = z\n",
    "if(params['num_dis_c'] != 0):\n",
    "    idx = np.arange(params['dis_c_dim']).repeat(10)\n",
    "    dis_c = torch.zeros(100, params['num_dis_c'], params['dis_c_dim'], device=device)\n",
    "    for i in range(params['num_dis_c']):\n",
    "        dis_c[torch.arange(0, 100), i, idx] = 1.0\n",
    "\n",
    "    dis_c = dis_c.view(100, -1)\n",
    "\n",
    "    # fixed_noise = torch.cat((fixed_noise, dis_c), dim=1)\n",
    "fixed_idx= torch.tensor(idx,device = device,dtype = torch.long).reshape(-1,1)\n",
    "if(params['num_con_c'] != 0):\n",
    "    con_c = torch.rand(100, params['num_con_c'],  device=device) * 2 - 1\n",
    "    # fixed_noise = torch.cat((fixed_noise, con_c), dim=1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0.0\n",
    "\n",
    "# List variables to store results pf training.\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise, idx, gt_fake, continuous = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], 100, device)\n",
    "# fake_data = netG(fixed_noise,fixed_idx,con_c)\n",
    "# noise = vae(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Starting Training Loop...\n",
      "\n",
      "Epochs: 500\n",
      "Dataset: HGD\n",
      "Batch Size: 64\n",
      "Length of Data Loader: 250\n",
      "-------------------------\n",
      "[1/500][100/250]\tLoss_D: 1.7576\tLoss_G: 7.9163\n",
      "[1/500][200/250]\tLoss_D: 0.7869\tLoss_G: 3.7035\n",
      "Time taken for Epoch 1: 13.59s\n",
      "[2/500][100/250]\tLoss_D: 1.3430\tLoss_G: 5.0726\n",
      "[2/500][200/250]\tLoss_D: 0.9557\tLoss_G: 1.9577\n",
      "Time taken for Epoch 2: 11.87s\n",
      "[3/500][100/250]\tLoss_D: 1.4861\tLoss_G: 5.1547\n",
      "[3/500][200/250]\tLoss_D: 0.9964\tLoss_G: 5.6024\n",
      "Time taken for Epoch 3: 11.46s\n",
      "[4/500][100/250]\tLoss_D: 1.4227\tLoss_G: 3.0293\n",
      "[4/500][200/250]\tLoss_D: 0.4683\tLoss_G: 3.9013\n",
      "Time taken for Epoch 4: 11.35s\n",
      "[5/500][100/250]\tLoss_D: 1.1732\tLoss_G: 3.9317\n",
      "[5/500][200/250]\tLoss_D: 0.9752\tLoss_G: 1.2063\n",
      "Time taken for Epoch 5: 11.40s\n",
      "[6/500][100/250]\tLoss_D: 0.8352\tLoss_G: 3.1001\n",
      "[6/500][200/250]\tLoss_D: 0.7507\tLoss_G: 1.2593\n",
      "Time taken for Epoch 6: 12.47s\n",
      "[7/500][100/250]\tLoss_D: 0.8700\tLoss_G: 4.8324\n",
      "[7/500][200/250]\tLoss_D: 0.6537\tLoss_G: 1.9809\n",
      "Time taken for Epoch 7: 12.99s\n",
      "[8/500][100/250]\tLoss_D: 0.6880\tLoss_G: 3.6562\n",
      "[8/500][200/250]\tLoss_D: 0.5399\tLoss_G: 5.4914\n",
      "Time taken for Epoch 8: 12.99s\n",
      "[9/500][100/250]\tLoss_D: 1.4463\tLoss_G: 5.8074\n",
      "[9/500][200/250]\tLoss_D: 0.5046\tLoss_G: 3.5484\n",
      "Time taken for Epoch 9: 12.73s\n",
      "[10/500][100/250]\tLoss_D: 0.0361\tLoss_G: 9.1726\n",
      "[10/500][200/250]\tLoss_D: 0.0212\tLoss_G: 40.5401\n",
      "Time taken for Epoch 10: 13.10s\n",
      "[11/500][100/250]\tLoss_D: 0.0092\tLoss_G: 10.8161\n",
      "[11/500][200/250]\tLoss_D: 0.3136\tLoss_G: 12.2227\n",
      "Time taken for Epoch 11: 12.65s\n",
      "[12/500][100/250]\tLoss_D: 0.2225\tLoss_G: 6.1484\n",
      "[12/500][200/250]\tLoss_D: 0.2728\tLoss_G: 4.1452\n",
      "Time taken for Epoch 12: 12.49s\n",
      "[13/500][100/250]\tLoss_D: 0.6204\tLoss_G: 5.0055\n",
      "[13/500][200/250]\tLoss_D: 0.3124\tLoss_G: 4.8801\n",
      "Time taken for Epoch 13: 12.26s\n",
      "[14/500][100/250]\tLoss_D: 0.2560\tLoss_G: 7.7554\n",
      "[14/500][200/250]\tLoss_D: 0.2475\tLoss_G: 3.7453\n",
      "Time taken for Epoch 14: 13.08s\n",
      "[15/500][100/250]\tLoss_D: 0.6603\tLoss_G: 8.5755\n",
      "[15/500][200/250]\tLoss_D: 0.4674\tLoss_G: 5.8096\n",
      "Time taken for Epoch 15: 12.89s\n",
      "[16/500][100/250]\tLoss_D: 1.8635\tLoss_G: 8.5915\n",
      "[16/500][200/250]\tLoss_D: 0.3881\tLoss_G: 4.6592\n",
      "Time taken for Epoch 16: 12.53s\n",
      "[17/500][100/250]\tLoss_D: 0.6326\tLoss_G: 1.9102\n",
      "[17/500][200/250]\tLoss_D: 0.4516\tLoss_G: 1.4509\n",
      "Time taken for Epoch 17: 12.60s\n",
      "[18/500][100/250]\tLoss_D: 0.3287\tLoss_G: 7.5000\n",
      "[18/500][200/250]\tLoss_D: 0.2776\tLoss_G: 2.2944\n",
      "Time taken for Epoch 18: 12.55s\n",
      "[19/500][100/250]\tLoss_D: 0.2511\tLoss_G: 3.2918\n",
      "[19/500][200/250]\tLoss_D: 0.0577\tLoss_G: 4.8952\n",
      "Time taken for Epoch 19: 13.03s\n",
      "[20/500][100/250]\tLoss_D: 0.9978\tLoss_G: 7.6642\n",
      "[20/500][200/250]\tLoss_D: 0.6349\tLoss_G: 2.0298\n",
      "Time taken for Epoch 20: 12.75s\n",
      "[21/500][100/250]\tLoss_D: 0.3431\tLoss_G: 4.0233\n",
      "[21/500][200/250]\tLoss_D: 0.2421\tLoss_G: 6.8860\n",
      "Time taken for Epoch 21: 12.16s\n",
      "[22/500][100/250]\tLoss_D: 0.3980\tLoss_G: 3.7349\n",
      "[22/500][200/250]\tLoss_D: 0.5523\tLoss_G: 4.8382\n",
      "Time taken for Epoch 22: 12.96s\n",
      "[23/500][100/250]\tLoss_D: 0.2913\tLoss_G: 2.3805\n",
      "[23/500][200/250]\tLoss_D: 0.5552\tLoss_G: 3.5318\n",
      "Time taken for Epoch 23: 12.73s\n",
      "[24/500][100/250]\tLoss_D: 0.3095\tLoss_G: 2.8238\n",
      "[24/500][200/250]\tLoss_D: 0.2478\tLoss_G: 4.1441\n",
      "Time taken for Epoch 24: 12.64s\n",
      "[25/500][100/250]\tLoss_D: 0.1214\tLoss_G: 3.8815\n",
      "[25/500][200/250]\tLoss_D: 0.1002\tLoss_G: 4.2812\n",
      "Time taken for Epoch 25: 12.62s\n",
      "[26/500][100/250]\tLoss_D: 0.3778\tLoss_G: 3.6095\n",
      "[26/500][200/250]\tLoss_D: 0.5019\tLoss_G: 5.8942\n",
      "Time taken for Epoch 26: 12.34s\n",
      "[27/500][100/250]\tLoss_D: 0.4380\tLoss_G: 6.6672\n",
      "[27/500][200/250]\tLoss_D: 0.5229\tLoss_G: 6.0902\n",
      "Time taken for Epoch 27: 11.75s\n",
      "[28/500][100/250]\tLoss_D: 0.2064\tLoss_G: 6.6027\n",
      "[28/500][200/250]\tLoss_D: 0.2362\tLoss_G: 3.0161\n",
      "Time taken for Epoch 28: 12.72s\n",
      "[29/500][100/250]\tLoss_D: 2.1101\tLoss_G: 11.7595\n",
      "[29/500][200/250]\tLoss_D: 0.2460\tLoss_G: 2.6654\n",
      "Time taken for Epoch 29: 11.83s\n",
      "[30/500][100/250]\tLoss_D: 0.3225\tLoss_G: 3.8320\n",
      "[30/500][200/250]\tLoss_D: 0.4011\tLoss_G: 5.6851\n",
      "Time taken for Epoch 30: 12.03s\n",
      "[31/500][100/250]\tLoss_D: 0.3073\tLoss_G: 3.6369\n",
      "[31/500][200/250]\tLoss_D: 0.5078\tLoss_G: 3.9515\n",
      "Time taken for Epoch 31: 11.85s\n",
      "[32/500][100/250]\tLoss_D: 0.2380\tLoss_G: 2.4107\n",
      "[32/500][200/250]\tLoss_D: 0.1983\tLoss_G: 2.9061\n",
      "Time taken for Epoch 32: 11.60s\n",
      "[33/500][100/250]\tLoss_D: 0.2435\tLoss_G: 2.9628\n",
      "[33/500][200/250]\tLoss_D: 0.9087\tLoss_G: 9.5051\n",
      "Time taken for Epoch 33: 11.52s\n",
      "[34/500][100/250]\tLoss_D: 0.7725\tLoss_G: 8.5712\n",
      "[34/500][200/250]\tLoss_D: 0.3993\tLoss_G: 2.7932\n",
      "Time taken for Epoch 34: 11.82s\n",
      "[35/500][100/250]\tLoss_D: 0.4573\tLoss_G: 2.8800\n",
      "[35/500][200/250]\tLoss_D: 0.2825\tLoss_G: 3.4907\n",
      "Time taken for Epoch 35: 13.29s\n",
      "[36/500][100/250]\tLoss_D: 0.3363\tLoss_G: 2.8655\n",
      "[36/500][200/250]\tLoss_D: 0.1935\tLoss_G: 2.6110\n",
      "Time taken for Epoch 36: 14.02s\n",
      "[37/500][100/250]\tLoss_D: 0.1615\tLoss_G: 4.8321\n",
      "[37/500][200/250]\tLoss_D: 0.1494\tLoss_G: 6.8340\n",
      "Time taken for Epoch 37: 13.57s\n",
      "[38/500][100/250]\tLoss_D: 0.2978\tLoss_G: 3.9841\n",
      "[38/500][200/250]\tLoss_D: 0.1731\tLoss_G: 2.7402\n",
      "Time taken for Epoch 38: 12.88s\n",
      "[39/500][100/250]\tLoss_D: 0.4149\tLoss_G: 4.3853\n",
      "[39/500][200/250]\tLoss_D: 0.4666\tLoss_G: 4.3180\n",
      "Time taken for Epoch 39: 13.02s\n",
      "[40/500][100/250]\tLoss_D: 0.3769\tLoss_G: 2.2252\n",
      "[40/500][200/250]\tLoss_D: 0.0839\tLoss_G: 3.8197\n",
      "Time taken for Epoch 40: 12.97s\n",
      "[41/500][100/250]\tLoss_D: 0.4202\tLoss_G: 1.1004\n",
      "[41/500][200/250]\tLoss_D: 0.3130\tLoss_G: 1.4497\n",
      "Time taken for Epoch 41: 13.30s\n",
      "[42/500][100/250]\tLoss_D: 0.3394\tLoss_G: 4.6144\n",
      "[42/500][200/250]\tLoss_D: 0.2012\tLoss_G: 2.6784\n",
      "Time taken for Epoch 42: 13.28s\n",
      "[43/500][100/250]\tLoss_D: 0.1860\tLoss_G: 7.4725\n",
      "[43/500][200/250]\tLoss_D: 0.1655\tLoss_G: 6.0290\n",
      "Time taken for Epoch 43: 13.78s\n",
      "[44/500][100/250]\tLoss_D: 0.3029\tLoss_G: 6.4267\n",
      "[44/500][200/250]\tLoss_D: 0.0902\tLoss_G: 4.0324\n",
      "Time taken for Epoch 44: 13.79s\n",
      "[45/500][100/250]\tLoss_D: 0.6340\tLoss_G: 4.3017\n",
      "[45/500][200/250]\tLoss_D: 0.1347\tLoss_G: 3.8987\n",
      "Time taken for Epoch 45: 13.68s\n",
      "[46/500][100/250]\tLoss_D: 0.1523\tLoss_G: 3.3657\n",
      "[46/500][200/250]\tLoss_D: 0.0771\tLoss_G: 8.7886\n",
      "Time taken for Epoch 46: 13.52s\n",
      "[47/500][100/250]\tLoss_D: 0.0206\tLoss_G: 6.4415\n",
      "[47/500][200/250]\tLoss_D: 0.2646\tLoss_G: 4.4389\n",
      "Time taken for Epoch 47: 12.92s\n",
      "[48/500][100/250]\tLoss_D: 0.0547\tLoss_G: 5.0118\n",
      "[48/500][200/250]\tLoss_D: 0.3183\tLoss_G: 4.6290\n",
      "Time taken for Epoch 48: 12.78s\n",
      "[49/500][100/250]\tLoss_D: 0.2003\tLoss_G: 5.4305\n",
      "[49/500][200/250]\tLoss_D: 0.0975\tLoss_G: 5.2761\n",
      "Time taken for Epoch 49: 12.63s\n",
      "[50/500][100/250]\tLoss_D: 0.1218\tLoss_G: 8.9426\n",
      "[50/500][200/250]\tLoss_D: 0.1559\tLoss_G: 6.6999\n",
      "Time taken for Epoch 50: 12.97s\n",
      "[51/500][100/250]\tLoss_D: 0.0820\tLoss_G: 5.7544\n",
      "[51/500][200/250]\tLoss_D: 0.2495\tLoss_G: 4.3943\n",
      "Time taken for Epoch 51: 12.85s\n",
      "[52/500][100/250]\tLoss_D: 0.4185\tLoss_G: 6.0390\n",
      "[52/500][200/250]\tLoss_D: 0.2243\tLoss_G: 4.1411\n",
      "Time taken for Epoch 52: 12.86s\n",
      "[53/500][100/250]\tLoss_D: 0.2147\tLoss_G: 3.7355\n",
      "[53/500][200/250]\tLoss_D: 0.2887\tLoss_G: 2.6924\n",
      "Time taken for Epoch 53: 12.67s\n",
      "[54/500][100/250]\tLoss_D: 0.1293\tLoss_G: 4.1952\n",
      "[54/500][200/250]\tLoss_D: 0.1545\tLoss_G: 8.3741\n",
      "Time taken for Epoch 54: 12.80s\n",
      "[55/500][100/250]\tLoss_D: 0.1052\tLoss_G: 5.1828\n",
      "[55/500][200/250]\tLoss_D: 0.1648\tLoss_G: 2.6419\n",
      "Time taken for Epoch 55: 12.24s\n",
      "[56/500][100/250]\tLoss_D: 0.1443\tLoss_G: 2.6521\n",
      "[56/500][200/250]\tLoss_D: 0.1281\tLoss_G: 4.7971\n",
      "Time taken for Epoch 56: 13.19s\n",
      "[57/500][100/250]\tLoss_D: 0.1923\tLoss_G: 5.2262\n",
      "[57/500][200/250]\tLoss_D: 0.0694\tLoss_G: 4.4525\n",
      "Time taken for Epoch 57: 13.63s\n",
      "[58/500][100/250]\tLoss_D: 0.3829\tLoss_G: 4.1912\n",
      "[58/500][200/250]\tLoss_D: 0.0334\tLoss_G: 4.5736\n",
      "Time taken for Epoch 58: 12.62s\n",
      "[59/500][100/250]\tLoss_D: 0.1574\tLoss_G: 5.7775\n",
      "[59/500][200/250]\tLoss_D: 0.1813\tLoss_G: 3.2344\n",
      "Time taken for Epoch 59: 12.08s\n",
      "[60/500][100/250]\tLoss_D: 0.0966\tLoss_G: 4.8679\n",
      "[60/500][200/250]\tLoss_D: 0.1580\tLoss_G: 3.4007\n",
      "Time taken for Epoch 60: 12.27s\n",
      "[61/500][100/250]\tLoss_D: 0.0913\tLoss_G: 4.4956\n",
      "[61/500][200/250]\tLoss_D: 0.1134\tLoss_G: 5.2971\n",
      "Time taken for Epoch 61: 12.41s\n",
      "[62/500][100/250]\tLoss_D: 0.0473\tLoss_G: 3.4845\n",
      "[62/500][200/250]\tLoss_D: 0.1975\tLoss_G: 5.7178\n",
      "Time taken for Epoch 62: 12.07s\n",
      "[63/500][100/250]\tLoss_D: 0.0480\tLoss_G: 3.6457\n",
      "[63/500][200/250]\tLoss_D: 0.0626\tLoss_G: 4.6088\n",
      "Time taken for Epoch 63: 11.97s\n",
      "[64/500][100/250]\tLoss_D: 0.1844\tLoss_G: 4.4228\n",
      "[64/500][200/250]\tLoss_D: 0.0930\tLoss_G: 2.7082\n",
      "Time taken for Epoch 64: 12.60s\n",
      "[65/500][100/250]\tLoss_D: 0.2084\tLoss_G: 5.2428\n",
      "[65/500][200/250]\tLoss_D: 0.1288\tLoss_G: 4.2110\n",
      "Time taken for Epoch 65: 12.37s\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"Starting Training Loop...\\n\")\n",
    "print('Epochs: %d\\nDataset: {}\\nBatch Size: %d\\nLength of Data Loader: %d'.format(params['dataset']) % (params['num_epochs'], params['batch_size'], len(dataloader)))\n",
    "print(\"-\"*25)\n",
    "\n",
    "start_time = time.time()\n",
    "iters = 0\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (data, gt_labels) in enumerate(dataloader, 0):\n",
    "        # Get batch size\n",
    "        b_size = data.size(0)\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        real_data = data.to(device)\n",
    "        gt_labels = gt_labels.unsqueeze(1).to(device,dtype = torch.long)\n",
    "\n",
    "        # Updating discriminator and DHead\n",
    "        optimD.zero_grad()\n",
    "        # Real data\n",
    "        label = torch.full((b_size, ), real_label, device=device,dtype=torch.float32)\n",
    "        # print(real_data.shape,'realdata')\n",
    "        # print(real_data.shape)\n",
    "        output1 = discriminator(real_data, gt_labels)\n",
    "        discriminator_weights = activation.copy()\n",
    "        # print(output1.shape,'output1')\n",
    "        # output_real_DG = netDG(output1)\n",
    "        # print(output_real_DG.shape,'dgshape')\n",
    "        # print(real_data.shape,output_real_DG.shape)\n",
    "        # loss_real_ae= criterion_mse(real_data,output_real_DG)\n",
    "        probs_real = netD(output1).view(-1)\n",
    "        # classifier,_,_ = netQ(output1)\n",
    "\n",
    "        # print(label.dtype,probs_real.dtype)\n",
    "        # print(classifier.shape, gt_labels.shape)\n",
    "        # lossR_class = criterionQ_dis(classifier,gt_labels.squeeze())\n",
    "        # print(classifier,gt_labels)\n",
    "        loss_real = criterionD(probs_real, label)\n",
    "        # Calculate gradients.\n",
    "        # loss_real+=loss_real_ae\n",
    "        lossR= loss_real# + lossR_class\n",
    "        lossR.backward()\n",
    "\n",
    "        # Fake data\n",
    "        label.fill_(fake_label)\n",
    "        _,idx, _, continuous = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], b_size, device)\n",
    "        z_real,real_mu,real_var = vae(real_data, gt_labels)\n",
    "        target = torch.LongTensor(idx).to(device)\n",
    "        fake_data = netG(z_real,gt_labels,continuous)\n",
    "        # print(fake_data.shape,'fake_data1')\n",
    "        output2 = discriminator(fake_data.detach(),gt_labels)\n",
    "        # classifier_fake,_,_ = netQ(output2)\n",
    "      \n",
    "        # print(classifier.shape,target_d.shape)\n",
    "        # output_fake_DG = netDG(output2)\n",
    "        # print(output_real_DG.shape,'dgshape')\n",
    "        # print(real_data.shape,output_real_DG.shape)\n",
    "        # loss_fake_ae= criterion_mse(fake_data.detach(),output_fake_DG)\n",
    "        # lossF_class = criterionQ_dis(classifier_fake,target.reshape(gt_labels.shape))\n",
    "\n",
    "\n",
    "        probs_fake = netD(output2).view(-1)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        # Calculate gradients.\n",
    "        # lossF =loss_fake + lossF_class# + loss_fake_ae\n",
    "        loss_fake.backward()\n",
    "       \n",
    "\n",
    "        # Net Loss for the discriminator\n",
    "        D_loss = loss_fake + loss_real\n",
    "        # Update parameters\n",
    "        optimD.step()\n",
    "\n",
    "        # Updating Generator and QHead\n",
    "        optimG.zero_grad()\n",
    "        \n",
    "        # Fake data treated as real.\n",
    "        # print(fake_data.shape,'fake data shpe')\n",
    "        output = discriminator(fake_data,gt_labels)\n",
    "        generator_weights = activation.copy()\n",
    "        # print(output, 'output discrim')\n",
    "        label.fill_(real_label)\n",
    "        # fake_logit = netLogit(output)\n",
    "        z_fake,fake_mu,fake_var = vae(fake_data,gt_labels)\n",
    "        loss_feat = 0\n",
    "        for key in generator_weights:\n",
    "            loss_feat += criterionLAT(discriminator_weights[key], generator_weights[key])\n",
    "        probs_fake = netD(output).view(-1)\n",
    "        #output_fake_DG = netDG(output)\n",
    "        #loss_fake_ae= criterion_mse(fake_data,output_fake_DG)\n",
    "        loss_gen = criterionD(probs_fake, label)\n",
    "        q_logits, q_mu, q_var = netQ(output)\n",
    "        loss_recon = criterionRECON(real_data,fake_data)\n",
    "        loss_latent = criterionLAT(z_real,z_fake)\n",
    "        # gen_dis_loss = criterionQ_dis(fake_logit,q_logits)\n",
    "\n",
    "        # Calculating loss for discrete latent code.\n",
    "        # dis_loss = 0\n",
    "        loss_dis = criterionQ_dis(q_logits,target.to(dtype=torch.long).squeeze())\n",
    "        # for j in range(params['num_dis_c']):\n",
    "        #     dis_loss += criterionQ_dis(q_logits[:, j*10 : j*10 + 10], target[j])\n",
    "\n",
    "        # Calculating loss for continuous latent code.\n",
    "        loss_con = 0\n",
    "        if (params['num_con_c'] != 0):\n",
    "            loss_con = criterionQ_con(continuous, q_mu, q_var)\n",
    "        # recon_loss = criterionRECON(real_data,fake_data)\n",
    "        # latent_loss = criterionLAT((real_mu+ torch.randn_like(real_var)*torch.sqrt(real_var.exp())).view(b_size,-1,1,1),\n",
    "        #                           (fake_mu+ torch.randn_like(fake_var)*torch.sqrt(fake_var.exp())).view(b_size,-1,1,1))\n",
    "        \n",
    "        loss_KLD  = torch.mean(-0.5 * torch.sum(1 + real_var - real_mu.pow(2) - real_var.exp(),dim=1)).to(device)\n",
    "        # Net loss for generator.\n",
    "        G_loss = loss_gen + loss_dis +  5*loss_recon + 0.5*loss_latent + 0.5*loss_con + 0.05*loss_KLD\n",
    "        # print(G_loss)\n",
    "        # Calculate gradients.\n",
    "        G_loss.backward()\n",
    "        # Update parameters.\n",
    "        # torch.nn.utils.clip_grad_norm_(netG.parameters(), 1)\n",
    "        optimG.step()\n",
    "\n",
    "        # Check progress of training.\n",
    "        if i != 0 and i%100 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch+1, params['num_epochs'], i, len(dataloader), \n",
    "                   D_loss.item(), G_loss.item()))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        G_losses.append(G_loss.item())\n",
    "        D_losses.append(D_loss.item())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(\"Time taken for Epoch %d: %.2fs\" %(epoch + 1, epoch_time))\n",
    "    # Generate image after each epoch to check performance of the generator. Used for creating animated gif later.\n",
    "    with torch.no_grad():\n",
    "        gen_data = netG(z,fixed_idx,con_c).detach().cpu()\n",
    "    img_list.append(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True))\n",
    "\n",
    "    # Generate image to check performance of generator.\n",
    "    if((epoch+1) == 1 or (epoch+1) % 10==0):\n",
    "        with torch.no_grad():\n",
    "            gen_data = netG(z,fixed_idx,con_c).detach().cpu()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "        plt.savefig(\"results/VAE/Epoch_%d_{}\".format(params['dataset']) %(epoch+1))\n",
    "        plt.close('all')\n",
    "\n",
    "    # Save network weights.\n",
    "    if (epoch+1) % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'netG' : netG.state_dict(),\n",
    "            'discriminator' : discriminator.state_dict(),\n",
    "            'netD' : netD.state_dict(),\n",
    "            'netQ' : netQ.state_dict(),\n",
    "            'optimD' : optimD.state_dict(),\n",
    "            'optimG' : optimG.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'checkpoint/model_vae_epoch_%d_{}'.format(params['dataset']) %(epoch+1))\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"-\"*50)\n",
    "print('Training finished!\\nTotal Time for Training: %.2fm' %(training_time / 60))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate image to check performance of trained generator.\n",
    "with torch.no_grad():\n",
    "    gen_data = netG(z,fixed_idx,con_c).detach().cpu()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.savefig(\"results/VAE/Epoch_%d_{}\".format(params['dataset']) %(params['num_epochs']))\n",
    "\n",
    "# Save network weights.\n",
    "torch.save({\n",
    "    'netG' : netG.state_dict(),\n",
    "    'discriminator' : discriminator.state_dict(),\n",
    "    'netD' : netD.state_dict(),\n",
    "    'netQ' : netQ.state_dict(),\n",
    "    'optimD' : optimD.state_dict(),\n",
    "    'optimG' : optimG.state_dict(),\n",
    "    'params' : params\n",
    "    }, 'checkpoint/model_VAE_final_{}'.format(params['dataset']))\n",
    "\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"results/VAE/Loss Curve {}\".format(params['dataset']))\n",
    "\n",
    "# Animation showing the improvements of the generator.\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "anim = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "anim.save('results/VAEinfoGAN_{}.gif'.format(params['dataset']), dpi=80, writer='imagemagick')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-load_path', required=True, help='Checkpoint to load path from')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "load_path = 'checkpoint/model_final_HGD2'\n",
    "\n",
    "from models.hgd_model2 import Generator\n",
    "\n",
    "# Load the checkpoint file\n",
    "state_dict = torch.load(load_path)\n",
    "\n",
    "# Set the device to run on: GPU or CPU.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "# Get the 'params' dictionary from the loaded state_dict.\n",
    "params = state_dict['params']\n",
    "\n",
    "# Create the generator network.\n",
    "netG = Generator().to(device)\n",
    "# Load the trained generator weights.\n",
    "netG.load_state_dict(state_dict['netG'])\n",
    "# print(netG)\n",
    "\n",
    "c = np.linspace(-2, 2, 10).reshape(1, -1)\n",
    "c = np.repeat(c, 10).reshape(-1, 1)\n",
    "c = torch.from_numpy(c).float().to(device)\n",
    "c = c.view(-1, 1)\n",
    "# print(c.shape,'cshape')\n",
    "zeros = torch.zeros(100, 1, device=device)\n",
    "# Continuous latent code.\n",
    "c2 = torch.cat((zeros, zeros,zeros,zeros), dim=1)\n",
    "c3 = torch.cat((zeros, c,zeros,zeros), dim=1)\n",
    "c4 = torch.cat((zeros,zeros,c, zeros), dim=1)\n",
    "c5 = torch.cat((zeros,zeros,zeros, c), dim=1)\n",
    "# print(c2)\n",
    "# print(c2.shape,'c2shape')\n",
    "idx = np.arange(10).repeat(10)\n",
    "dis_c = torch.zeros(100, 8, 10, 1, device=device)\n",
    "for i in range(8):\n",
    "    dis_c[torch.arange(0, 100),i, idx] = .9\n",
    "# Discrete latent code.\n",
    "c1 = dis_c.view(100, -1, 1, 1)\n",
    "# print(c1,'c1')\n",
    "z = torch.randn(100, 256, device=device)\n",
    "idx = torch.tensor(idx,device=device,dtype=torch.long).unsqueeze(1)\n",
    "# print(z.dtype,idx.dtype,c2.dtype)\n",
    "# To see variation along c2 (Horizontally) and c1 (Vertically)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img1 = netG(z,idx,c2).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img1, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img2 = netG(z,idx,c3).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img2, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_img1 = netG(z,idx,c4).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img1, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img2 = netG(z,idx,c5).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img2, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoGan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
