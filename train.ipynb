{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import random\n",
    "\n",
    "from models.svhn_model import Generator, Discriminator, DHead, QHead, VAEncoder\n",
    "from dataloader import get_data\n",
    "from utils import *\n",
    "from config import params\n",
    "\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    from models.svhn_model import Generator, Discriminator, DHead, QHead, VAEncoder\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    from models.celeba_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'HGD'):\n",
    "    from models.hgd_model import Generator, Discriminator, DHead, QHead,VAEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1123\n",
      "cuda:0  will be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility.\n",
    "seed = 1123\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "# Use GPU if available.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")\n",
    "\n",
    "dataloader = get_data(params['dataset'], params['batch_size'])\n",
    "\n",
    "# Set appropriate hyperparameters depending on the dataset used.\n",
    "# The values given in the InfoGAN paper are used.\n",
    "# num_z : dimension of incompressible noise.\n",
    "# num_dis_c : number of discrete latent code used.\n",
    "# dis_c_dim : dimension of discrete latent code.\n",
    "# num_con_c : number of continuous latent code used.\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    params['num_z'] = 124\n",
    "    params['num_dis_c'] = 4\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 4\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    params['num_z'] = 128\n",
    "    params['num_dis_c'] = 10\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 0\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "\n",
    "# Plot the training images.\n",
    "sample_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(\n",
    "    sample_batch[0].to(device)[ : 100], nrow=10, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "plt.savefig('results\\Training Images {}'.format(params['dataset']))\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (tconv1): ConvTranspose2d(168, 448, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv2): ConvTranspose2d(448, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (tconv4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (tconv5): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "DHead(\n",
      "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n",
      "QHead(\n",
      "  (conv1): Conv2d(256, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_disc): Conv2d(128, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_mu): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "VAEncoder(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_mu): Sequential(\n",
      "    (0): Linear(in_features=12544, out_features=3136, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3136, out_features=124, bias=True)\n",
      "  )\n",
      "  (fc_var): Sequential(\n",
      "    (0): Linear(in_features=12544, out_features=3136, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3136, out_features=124, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the network.\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "print(discriminator)\n",
    "\n",
    "netD = DHead().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "netQ = QHead().to(device)\n",
    "netQ.apply(weights_init)\n",
    "print(netQ)\n",
    "\n",
    "vae = VAEncoder().to(device)\n",
    "vae.apply(weights_init)\n",
    "print(vae)\n",
    "# Loss for discrimination between real and fake images.\n",
    "criterionD = nn.BCELoss()\n",
    "# Loss for discrete latent code.\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "# Loss for continuous latent code.\n",
    "criterionQ_con = NormalNLLLoss()\n",
    "# Loss for image\n",
    "criterionRECON = nn.L1Loss()\n",
    "# Loss for latent encoding\n",
    "criterionLAT = nn.L1Loss()\n",
    "# Adam optimiser is used.\n",
    "optimD = optim.Adam([{'params': discriminator.parameters()}, {'params': netD.parameters()}], lr=params['learning_rate_d'], betas=(params['beta1'], params['beta2']))\n",
    "optimG = optim.Adam([{'params': vae.parameters()},{'params': netG.parameters()}, {'params': netQ.parameters()}], lr=params['learning_rate_g'], betas=(params['beta1'], params['beta2']))\n",
    "\n",
    "# Fixed Noise\n",
    "z = torch.randn(100, params['num_z'], 1, 1, device=device)\n",
    "fixed_noise = z\n",
    "if(params['num_dis_c'] != 0):\n",
    "    idx = np.arange(params['dis_c_dim']).repeat(10)\n",
    "    dis_c = torch.zeros(100, params['num_dis_c'], params['dis_c_dim'], device=device)\n",
    "    for i in range(params['num_dis_c']):\n",
    "        dis_c[torch.arange(0, 100), i, idx] = 1.0\n",
    "\n",
    "    dis_c = dis_c.view(100, -1, 1, 1)\n",
    "\n",
    "    fixed_noise = torch.cat((fixed_noise, dis_c), dim=1)\n",
    "\n",
    "if(params['num_con_c'] != 0):\n",
    "    con_c = torch.rand(100, params['num_con_c'], 1, 1, device=device) * 2 - 1\n",
    "    fixed_noise = torch.cat((fixed_noise, con_c), dim=1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# List variables to store results pf training.\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Starting Training Loop...\n",
      "\n",
      "Epochs: 100\n",
      "Dataset: SVHN\n",
      "Batch Size: 128\n",
      "Length of Data Loader: 125\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(params[\u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m     10\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mfor\u001b[39;00m i, (data, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(dataloader, \u001b[39m0\u001b[39;49m):\n\u001b[0;32m     13\u001b[0m         \u001b[39m# Get batch size\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         b_size \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m         \u001b[39m# Transfer data tensor to GPU/CPU (device)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:442\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1043\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1036\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"Starting Training Loop...\\n\")\n",
    "print('Epochs: %d\\nDataset: {}\\nBatch Size: %d\\nLength of Data Loader: %d'.format(params['dataset']) % (params['num_epochs'], params['batch_size'], len(dataloader)))\n",
    "print(\"-\"*25)\n",
    "\n",
    "start_time = time.time()\n",
    "iters = 0\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (data, _) in enumerate(dataloader, 0):\n",
    "        # Get batch size\n",
    "        b_size = data.size(0)\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        real_data = data.to(device)\n",
    "\n",
    "        # Updating discriminator and DHead\n",
    "        optimD.zero_grad()\n",
    "        # Real data\n",
    "        label = torch.full((b_size, ), real_label, device=device,dtype=torch.float32)\n",
    "        output1 = discriminator(real_data)\n",
    "        # print(output1.dtype)\n",
    "        # with torch.no_grad():\n",
    "        #     print(netD(output1).dtype)\n",
    "        probs_real = netD(output1).view(-1)\n",
    "        # print(label.dtype,probs_real.dtype)\n",
    "        loss_real = criterionD(probs_real, label)\n",
    "        # Calculate gradients.\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Fake data\n",
    "        label.fill_(fake_label)\n",
    "        # _, idx = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], b_size, device)\n",
    "        vae_mu_real,vae_var_real= vae(real_data)\n",
    "        noise = torch.normal(vae_mu_real,vae_var_real.exp()).view(b_size,-1,1,1)\n",
    "        idx = np.zeros((params['num_dis_c'], b_size))\n",
    "        if(params['num_dis_c'] != 0):\n",
    "            dis_c = torch.zeros(b_size, params['num_dis_c'], params['dis_c_dim'], device=device)\n",
    "            for i in range(params['num_dis_c']):\n",
    "                idx[i] = np.random.randint(params['dis_c_dim'], size=b_size)\n",
    "                dis_c[torch.arange(0, b_size), i, idx] = 1.0\n",
    "\n",
    "            dis_c = dis_c.view(b_size, -1, 1, 1)\n",
    "\n",
    "            noise = torch.cat((noise, dis_c), dim=1)\n",
    "\n",
    "        if(params['num_con_c'] != 0):\n",
    "            con_c = torch.rand(b_size, params['num_con_c'], 1, 1, device=device) * 2 - 1\n",
    "            noise = torch.cat((noise, con_c), dim=1)\n",
    "        fake_data = netG(noise)\n",
    "        output2 = discriminator(fake_data.detach())\n",
    "        probs_fake = netD(output2).view(-1)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        # Calculate gradients.\n",
    "        loss_fake.backward()\n",
    "\n",
    "        # Net Loss for the discriminator\n",
    "        D_loss = loss_real + loss_fake\n",
    "        # Update parameters\n",
    "        optimD.step()\n",
    "\n",
    "        # Updating Generator and QHead\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        # Fake data treated as real.\n",
    "        output = discriminator(fake_data)\n",
    "        vae_mu_fake,vae_var_fake = vae(fake_data)\n",
    "        label.fill_(real_label)\n",
    "        probs_fake = netD(output).view(-1)\n",
    "        gen_loss = criterionD(probs_fake, label)\n",
    "        recon_loss = criterionRECON(fake_data,real_data)\n",
    "        q_logits, q_mu, q_var = netQ(output)\n",
    "        target = torch.LongTensor(idx).to(device)\n",
    "        \n",
    "        \n",
    "        # Calculating loss for discrete latent code.\n",
    "        dis_loss = 0\n",
    "        for j in range(params['num_dis_c']):\n",
    "            dis_loss += criterionQ_dis(q_logits[:, j*10 : j*10 + 10], target[j])\n",
    "\n",
    "        # Calculating loss for continuous latent code.\n",
    "        con_loss = 0\n",
    "        if (params['num_con_c'] != 0):\n",
    "            con_loss = criterionQ_con(noise[:, params['num_z']+ params['num_dis_c']*params['dis_c_dim'] : ].view(-1, params['num_con_c']), q_mu, q_var)\n",
    "\n",
    "        latent_loss = criterionLAT(torch.normal(vae_mu_real,torch.exp(vae_var_real)).view(b_size,-1,1,1),\\\n",
    "                                   torch.normal(vae_mu_fake,torch.exp(vae_var_fake)).view(b_size,-1,1,1))\n",
    "        \n",
    "        KLD_loss  = torch.mean(-0.5 * torch.sum(1 + vae_var_real - vae_mu_real.pow(2) - vae_var_real.exp(),dim=1)).to(device)\n",
    "        # print(KLD_loss)\n",
    "        # Net loss for generator.\n",
    "        G_loss = gen_loss + dis_loss + con_loss + recon_loss + latent_loss + KLD_loss#*0.01\n",
    "        # Calculate gradients.\n",
    "        G_loss.backward()\n",
    "        # Update parameters.\n",
    "        optimG.step()\n",
    "\n",
    "        # Check progress of training.\n",
    "        if i != 0 and i%100 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch+1, params['num_epochs'], i, len(dataloader), \n",
    "                    D_loss.item(), G_loss.item()))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        G_losses.append(G_loss.item())\n",
    "        D_losses.append(D_loss.item())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(\"Time taken for Epoch %d: %.2fs\" %(epoch + 1, epoch_time))\n",
    "    # Generate image after each epoch to check performance of the generator. Used for creating animated gif later.\n",
    "    with torch.no_grad():\n",
    "        gen_data = netG(fixed_noise).detach().cpu()\n",
    "    img_list.append(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True))\n",
    "\n",
    "    # Generate image to check performance of generator.\n",
    "    if((epoch+1) == 1 or (epoch+1) == params['num_epochs']/2):\n",
    "        with torch.no_grad():\n",
    "            gen_data = netG(fixed_noise).detach().cpu()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "        plt.savefig(\"results\\Epoch_%d {}\".format(params['dataset']) %(epoch+1))\n",
    "        plt.close('all')\n",
    "\n",
    "    # Save network weights.\n",
    "    if (epoch+1) % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'netG' : netG.state_dict(),\n",
    "            'discriminator' : discriminator.state_dict(),\n",
    "            'netD' : netD.state_dict(),\n",
    "            'netQ' : netQ.state_dict(),\n",
    "            'optimD' : optimD.state_dict(),\n",
    "            'optimG' : optimG.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'checkpoint/model_epoch_%d_{}'.format(params['dataset']) %(epoch+1))\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"-\"*50)\n",
    "print('Training finished!\\nTotal Time for Training: %.2fm' %(training_time / 60))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(output1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate image to check performance of trained generator.\n",
    "with torch.no_grad():\n",
    "    gen_data = netG(fixed_noise).detach().cpu()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.savefig(\"results\\Epoch_%d_{}\".format(params['dataset']) %(params['num_epochs']))\n",
    "\n",
    "# Save network weights.\n",
    "torch.save({\n",
    "    'netG' : netG.state_dict(),\n",
    "    'discriminator' : discriminator.state_dict(),\n",
    "    'netD' : netD.state_dict(),\n",
    "    'netQ' : netQ.state_dict(),\n",
    "    'optimD' : optimD.state_dict(),\n",
    "    'optimG' : optimG.state_dict(),\n",
    "    'params' : params\n",
    "    }, 'checkpoint/model_final_{}'.format(params['dataset']))\n",
    "\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Loss Curve {}\".format(params['dataset']))\n",
    "\n",
    "# Animation showing the improvements of the generator.\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "anim = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "anim.save('results\\infoGAN_{}.gif'.format(params['dataset']), dpi=80, writer='imagemagick')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-load_path', required=True, help='Checkpoint to load path from')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "from models.svhn_model import Generator\n",
    "\n",
    "# Load the checkpoint file\n",
    "state_dict = torch.load('.\\checkpoint\\model_final_SVHN')\n",
    "\n",
    "# Set the device to run on: GPU or CPU.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "# Get the 'params' dictionary from the loaded state_dict.\n",
    "params = state_dict['params']\n",
    "\n",
    "# Create the generator network.\n",
    "netG = Generator().to(device)\n",
    "# Load the trained generator weights.\n",
    "netG.load_state_dict(state_dict['netG'])\n",
    "print(netG)\n",
    "\n",
    "c = np.linspace(-2, 2, 10).reshape(1, -1)\n",
    "c = np.repeat(c, 10, 0).reshape(-1, 1)\n",
    "c = torch.from_numpy(c).float().to(device)\n",
    "c = c.view(-1, 1, 1, 1)\n",
    "\n",
    "zeros = torch.zeros(100, 1, 1, 1, device=device)\n",
    "\n",
    "# Continuous latent code.\n",
    "c2 = torch.cat((c, zeros,c,zeros), dim=1)\n",
    "c3 = torch.cat((zeros, c,zeros,c), dim=1)\n",
    "\n",
    "idx = np.arange(10).repeat(10)\n",
    "dis_c = torch.zeros(100, 10, 4, 1, device=device)\n",
    "dis_c[torch.arange(0, 100), idx] = 1.0\n",
    "# Discrete latent code.\n",
    "c1 = dis_c.view(100, -1, 1, 1)\n",
    "\n",
    "z = torch.randn(100, 124, 1, 1, device=device)\n",
    "\n",
    "# To see variation along c2 (Horizontally) and c1 (Vertically)\n",
    "noise1 = torch.cat((z, c1, c2), dim=1)\n",
    "# To see variation along c3 (Horizontally) and c1 (Vertically)\n",
    "noise2 = torch.cat((z, c1, c3), dim=1)\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img1 = netG(noise1).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img1, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img2 = netG(noise2).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img2, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoGan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
