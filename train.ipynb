{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import random\n",
    "\n",
    "from models.svhn_model import Generator, Discriminator, DHead, QHead, VAEncoder\n",
    "from dataloader import get_data\n",
    "from utils import *\n",
    "from config import params\n",
    "\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    from models.svhn_model import Generator, Discriminator, DHead, QHead, VAEncoder\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    from models.celeba_model import Generator, Discriminator, DHead, QHead\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    from models.mnist_model import Generator, Discriminator, DHead, QHead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1123\n",
      "cuda:0  will be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility.\n",
    "seed = 1123\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "# Use GPU if available.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")\n",
    "\n",
    "dataloader = get_data(params['dataset'], params['batch_size'])\n",
    "\n",
    "# Set appropriate hyperparameters depending on the dataset used.\n",
    "# The values given in the InfoGAN paper are used.\n",
    "# num_z : dimension of incompressible noise.\n",
    "# num_dis_c : number of discrete latent code used.\n",
    "# dis_c_dim : dimension of discrete latent code.\n",
    "# num_con_c : number of continuous latent code used.\n",
    "if(params['dataset'] == 'MNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "elif(params['dataset'] == 'SVHN'):\n",
    "    params['num_z'] = 124\n",
    "    params['num_dis_c'] = 4\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 4\n",
    "elif(params['dataset'] == 'CelebA'):\n",
    "    params['num_z'] = 128\n",
    "    params['num_dis_c'] = 10\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 0\n",
    "elif(params['dataset'] == 'FashionMNIST'):\n",
    "    params['num_z'] = 62\n",
    "    params['num_dis_c'] = 1\n",
    "    params['dis_c_dim'] = 10\n",
    "    params['num_con_c'] = 2\n",
    "\n",
    "# Plot the training images.\n",
    "sample_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(\n",
    "    sample_batch[0].to(device)[ : 100], nrow=10, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "plt.savefig('results\\Training Images {}'.format(params['dataset']))\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (tconv1): ConvTranspose2d(168, 448, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv2): ConvTranspose2d(448, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (tconv4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (tconv5): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "DHead(\n",
      "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n",
      "QHead(\n",
      "  (conv1): Conv2d(256, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_disc): Conv2d(128, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_mu): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "VAEncoder(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_mu): Sequential(\n",
      "    (0): Linear(in_features=57600, out_features=3136, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3136, out_features=124, bias=True)\n",
      "  )\n",
      "  (fc_var): Sequential(\n",
      "    (0): Linear(in_features=57600, out_features=3136, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3136, out_features=124, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the network.\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "print(discriminator)\n",
    "\n",
    "netD = DHead().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "netQ = QHead().to(device)\n",
    "netQ.apply(weights_init)\n",
    "print(netQ)\n",
    "\n",
    "vae = VAEncoder().to(device)\n",
    "vae.apply(weights_init)\n",
    "print(vae)\n",
    "# Loss for discrimination between real and fake images.\n",
    "criterionD = nn.BCELoss()\n",
    "# Loss for discrete latent code.\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "# Loss for continuous latent code.\n",
    "criterionQ_con = NormalNLLLoss()\n",
    "# Loss for image\n",
    "criterionRECON = nn.L1Loss()\n",
    "# Loss for latent encoding\n",
    "criterionLAT = nn.L1Loss()\n",
    "# Adam optimiser is used.\n",
    "optimD = optim.Adam([{'params': discriminator.parameters()}, {'params': netD.parameters()}], lr=params['learning_rate_d'], betas=(params['beta1'], params['beta2']))\n",
    "optimG = optim.Adam([{'params': vae.parameters()},{'params': netG.parameters()}, {'params': netQ.parameters()}], lr=params['learning_rate_g'], betas=(params['beta1'], params['beta2']))\n",
    "\n",
    "# Fixed Noise\n",
    "z = torch.randn(100, params['num_z'], 1, 1, device=device)\n",
    "fixed_noise = z\n",
    "if(params['num_dis_c'] != 0):\n",
    "    idx = np.arange(params['dis_c_dim']).repeat(10)\n",
    "    dis_c = torch.zeros(100, params['num_dis_c'], params['dis_c_dim'], device=device)\n",
    "    for i in range(params['num_dis_c']):\n",
    "        dis_c[torch.arange(0, 100), i, idx] = 1.0\n",
    "\n",
    "    dis_c = dis_c.view(100, -1, 1, 1)\n",
    "\n",
    "    fixed_noise = torch.cat((fixed_noise, dis_c), dim=1)\n",
    "\n",
    "if(params['num_con_c'] != 0):\n",
    "    con_c = torch.rand(100, params['num_con_c'], 1, 1, device=device) * 2 - 1\n",
    "    fixed_noise = torch.cat((fixed_noise, con_c), dim=1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# List variables to store results pf training.\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Starting Training Loop...\n",
      "\n",
      "Epochs: 100\n",
      "Dataset: SVHN\n",
      "Batch Size: 128\n",
      "Length of Data Loader: 125\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128])) that is different to the input size (torch.Size([3200])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m probs_real \u001b[39m=\u001b[39m netD(output1)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39m# print(label.dtype,probs_real.dtype)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m loss_real \u001b[39m=\u001b[39m criterionD(probs_real, label)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Calculate gradients.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m loss_real\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Krishanu\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([3200])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"Starting Training Loop...\\n\")\n",
    "print('Epochs: %d\\nDataset: {}\\nBatch Size: %d\\nLength of Data Loader: %d'.format(params['dataset']) % (params['num_epochs'], params['batch_size'], len(dataloader)))\n",
    "print(\"-\"*25)\n",
    "\n",
    "start_time = time.time()\n",
    "iters = 0\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (data, _) in enumerate(dataloader, 0):\n",
    "        # Get batch size\n",
    "        b_size = data.size(0)\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        real_data = data.to(device)\n",
    "\n",
    "        # Updating discriminator and DHead\n",
    "        optimD.zero_grad()\n",
    "        # Real data\n",
    "        label = torch.full((b_size, ), real_label, device=device,dtype=torch.float32)\n",
    "        output1 = discriminator(real_data)\n",
    "        # print(output1.dtype)\n",
    "        # with torch.no_grad():\n",
    "        #     print(netD(output1).dtype)\n",
    "        probs_real = netD(output1).view(-1)\n",
    "        # print(label.dtype,probs_real.dtype)\n",
    "        loss_real = criterionD(probs_real, label)\n",
    "        # Calculate gradients.\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Fake data\n",
    "        label.fill_(fake_label)\n",
    "        # _, idx = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], b_size, device)\n",
    "        vae_mu_real,vae_var_real= vae(real_data)\n",
    "        noise = torch.normal(vae_mu_real,vae_var_real.exp()).view(b_size,-1,1,1)\n",
    "        idx = np.zeros((params['num_dis_c'], b_size))\n",
    "        if(params['num_dis_c'] != 0):\n",
    "            dis_c = torch.zeros(b_size, params['num_dis_c'], params['dis_c_dim'], device=device)\n",
    "            for i in range(params['num_dis_c']):\n",
    "                idx[i] = np.random.randint(params['dis_c_dim'], size=b_size)\n",
    "                dis_c[torch.arange(0, b_size), i, idx] = 1.0\n",
    "\n",
    "            dis_c = dis_c.view(b_size, -1, 1, 1)\n",
    "\n",
    "            noise = torch.cat((noise, dis_c), dim=1)\n",
    "\n",
    "        if(params['num_con_c'] != 0):\n",
    "            con_c = torch.rand(b_size, params['num_con_c'], 1, 1, device=device) * 2 - 1\n",
    "            noise = torch.cat((noise, con_c), dim=1)\n",
    "        fake_data = netG(noise)\n",
    "        output2 = discriminator(fake_data.detach())\n",
    "        probs_fake = netD(output2).view(-1)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        # Calculate gradients.\n",
    "        loss_fake.backward()\n",
    "\n",
    "        # Net Loss for the discriminator\n",
    "        D_loss = loss_real + loss_fake\n",
    "        # Update parameters\n",
    "        optimD.step()\n",
    "\n",
    "        # Updating Generator and QHead\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        # Fake data treated as real.\n",
    "        output = discriminator(fake_data)\n",
    "        vae_mu_fake,vae_var_fake = vae(fake_data)\n",
    "        label.fill_(real_label)\n",
    "        probs_fake = netD(output).view(-1)\n",
    "        gen_loss = criterionD(probs_fake, label)\n",
    "        recon_loss = criterionRECON(fake_data,real_data)\n",
    "        q_logits, q_mu, q_var = netQ(output)\n",
    "        target = torch.LongTensor(idx).to(device)\n",
    "        \n",
    "        \n",
    "        # Calculating loss for discrete latent code.\n",
    "        dis_loss = 0\n",
    "        for j in range(params['num_dis_c']):\n",
    "            dis_loss += criterionQ_dis(q_logits[:, j*10 : j*10 + 10], target[j])\n",
    "\n",
    "        # Calculating loss for continuous latent code.\n",
    "        con_loss = 0\n",
    "        if (params['num_con_c'] != 0):\n",
    "            con_loss = criterionQ_con(noise[:, params['num_z']+ params['num_dis_c']*params['dis_c_dim'] : ].view(-1, params['num_con_c']), q_mu, q_var)\n",
    "\n",
    "        latent_loss = criterionLAT(torch.normal(vae_mu_real,torch.exp(vae_var_real)).view(b_size,-1,1,1),\\\n",
    "                                   torch.normal(vae_mu_fake,torch.exp(vae_var_fake)).view(b_size,-1,1,1))\n",
    "        \n",
    "        KLD_loss  = torch.mean(-0.5 * torch.sum(1 + vae_var_real - vae_mu_real.pow(2) - vae_var_real.exp(),dim=1)).to(device)\n",
    "        # print(KLD_loss)\n",
    "        # Net loss for generator.\n",
    "        G_loss = gen_loss + dis_loss + con_loss + recon_loss + latent_loss + KLD_loss#*0.01\n",
    "        # Calculate gradients.\n",
    "        G_loss.backward()\n",
    "        # Update parameters.\n",
    "        optimG.step()\n",
    "\n",
    "        # Check progress of training.\n",
    "        if i != 0 and i%100 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch+1, params['num_epochs'], i, len(dataloader), \n",
    "                    D_loss.item(), G_loss.item()))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        G_losses.append(G_loss.item())\n",
    "        D_losses.append(D_loss.item())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(\"Time taken for Epoch %d: %.2fs\" %(epoch + 1, epoch_time))\n",
    "    # Generate image after each epoch to check performance of the generator. Used for creating animated gif later.\n",
    "    with torch.no_grad():\n",
    "        gen_data = netG(fixed_noise).detach().cpu()\n",
    "    img_list.append(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True))\n",
    "\n",
    "    # Generate image to check performance of generator.\n",
    "    if((epoch+1) == 1 or (epoch+1) == params['num_epochs']/2):\n",
    "        with torch.no_grad():\n",
    "            gen_data = netG(fixed_noise).detach().cpu()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "        plt.savefig(\"results\\Epoch_%d {}\".format(params['dataset']) %(epoch+1))\n",
    "        plt.close('all')\n",
    "\n",
    "    # Save network weights.\n",
    "    if (epoch+1) % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'netG' : netG.state_dict(),\n",
    "            'discriminator' : discriminator.state_dict(),\n",
    "            'netD' : netD.state_dict(),\n",
    "            'netQ' : netQ.state_dict(),\n",
    "            'optimD' : optimD.state_dict(),\n",
    "            'optimG' : optimG.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'checkpoint/model_epoch_%d_{}'.format(params['dataset']) %(epoch+1))\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"-\"*50)\n",
    "print('Training finished!\\nTotal Time for Training: %.2fm' %(training_time / 60))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate image to check performance of trained generator.\n",
    "with torch.no_grad():\n",
    "    gen_data = netG(fixed_noise).detach().cpu()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.savefig(\"results\\Epoch_%d_{}\".format(params['dataset']) %(params['num_epochs']))\n",
    "\n",
    "# Save network weights.\n",
    "torch.save({\n",
    "    'netG' : netG.state_dict(),\n",
    "    'discriminator' : discriminator.state_dict(),\n",
    "    'netD' : netD.state_dict(),\n",
    "    'netQ' : netQ.state_dict(),\n",
    "    'optimD' : optimD.state_dict(),\n",
    "    'optimG' : optimG.state_dict(),\n",
    "    'params' : params\n",
    "    }, 'checkpoint/model_final_{}'.format(params['dataset']))\n",
    "\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Loss Curve {}\".format(params['dataset']))\n",
    "\n",
    "# Animation showing the improvements of the generator.\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "anim = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "anim.save('results\\infoGAN_{}.gif'.format(params['dataset']), dpi=80, writer='imagemagick')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-load_path', required=True, help='Checkpoint to load path from')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "from models.svhn_model import Generator\n",
    "\n",
    "# Load the checkpoint file\n",
    "state_dict = torch.load('.\\checkpoint\\model_final_SVHN')\n",
    "\n",
    "# Set the device to run on: GPU or CPU.\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "# Get the 'params' dictionary from the loaded state_dict.\n",
    "params = state_dict['params']\n",
    "\n",
    "# Create the generator network.\n",
    "netG = Generator().to(device)\n",
    "# Load the trained generator weights.\n",
    "netG.load_state_dict(state_dict['netG'])\n",
    "print(netG)\n",
    "\n",
    "c = np.linspace(-2, 2, 10).reshape(1, -1)\n",
    "c = np.repeat(c, 10, 0).reshape(-1, 1)\n",
    "c = torch.from_numpy(c).float().to(device)\n",
    "c = c.view(-1, 1, 1, 1)\n",
    "\n",
    "zeros = torch.zeros(100, 1, 1, 1, device=device)\n",
    "\n",
    "# Continuous latent code.\n",
    "c2 = torch.cat((c, zeros,c,zeros), dim=1)\n",
    "c3 = torch.cat((zeros, c,zeros,c), dim=1)\n",
    "\n",
    "idx = np.arange(10).repeat(10)\n",
    "dis_c = torch.zeros(100, 10, 4, 1, device=device)\n",
    "dis_c[torch.arange(0, 100), idx] = 1.0\n",
    "# Discrete latent code.\n",
    "c1 = dis_c.view(100, -1, 1, 1)\n",
    "\n",
    "z = torch.randn(100, 124, 1, 1, device=device)\n",
    "\n",
    "# To see variation along c2 (Horizontally) and c1 (Vertically)\n",
    "noise1 = torch.cat((z, c1, c2), dim=1)\n",
    "# To see variation along c3 (Horizontally) and c1 (Vertically)\n",
    "noise2 = torch.cat((z, c1, c3), dim=1)\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img1 = netG(noise1).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img1, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "# Generate image.\n",
    "with torch.no_grad():\n",
    "    generated_img2 = netG(noise2).detach().cpu()\n",
    "# Display the generated image.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(generated_img2, nrow=10, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoGan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
