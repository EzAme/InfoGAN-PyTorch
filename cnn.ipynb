{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "load_path = 'checkpoint/model_final_HGD'\n",
    "\n",
    "from models.cnnmodel import Net\n",
    "from models.hgd_model2 import Generator\n",
    "from HandGestureDataset import HandGestureDataSet as HGD\n",
    "from utils import HGDThreshold as HGDT\n",
    "from utils import noise_sample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Krishanu/Desktop/Education/Programcodes/Python/DeepLearning/Project/leapGestRecog/\"\n",
    "training = HGD(root = path, train = True,\n",
    "    transform = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "              \n",
    "                # T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Resize((240,240)),\n",
    "                # T.CenterCrop((190,180)),\n",
    "                T.RandomCrop((180,180)),\n",
    "                T.Resize((64,64)),\n",
    "                T.RandomRotation(20),\n",
    "                T.ToTensor(), \n",
    "                HGDT(55.0/256.0),\n",
    "                # T.Normalize(100/256.0,1),\n",
    "                # T.RandomAdjustSharpness(sharpness_factor = 4,p=0.5),\n",
    "                # T.RandomAutocontrast(p=1),\n",
    "\n",
    "                ])\n",
    "            )\n",
    "validation = HGD(root = path, train= False,\n",
    "    transform = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "              \n",
    "                # T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Resize((240,240)),\n",
    "                # T.CenterCrop((190,180)),\n",
    "                T.RandomCrop((180,180)),\n",
    "                T.Resize((64,64)),\n",
    "                T.RandomRotation(20),\n",
    "                T.ToTensor(), \n",
    "                HGDT(55.0/256.0),\n",
    "                # T.Normalize(100/256.0,1),\n",
    "                # T.RandomAdjustSharpness(sharpness_factor = 4,p=0.5),\n",
    "                # T.RandomAutocontrast(p=1),\n",
    "\n",
    "                ])\n",
    "            )\n",
    "\n",
    "batch_size = 32\n",
    "train_batch = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers= 8)\n",
    "val_batch = DataLoader(validation, batch_size=batch_size, shuffle=True, num_workers= 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data, device, optimizer, epoch, netG, use_gen=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the main function for training a deep neural network.\n",
    "    Inputs:\n",
    "    {\n",
    "    model: The Neural network\n",
    "    training_data: Training data with labels\n",
    "    device: Physical location of where data is stored (\"CPU\" or \"GPU\")\n",
    "    optimizer: Optimizer Function e.g. torch.optim.adam\n",
    "    scheduler: The type of scheduling for modifying the learning rate\n",
    "    num_epochs: number of iterations to train on the data\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    model.train() #Set the model to \"training\" mode and compute gradients\n",
    "    for batch_idx, (image, label) in enumerate(training_data):\n",
    "        image, label = image.to(device), label.to(device) # place the input data into gpu ram or cpu ram\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = nn.functional.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            total_loss += loss.sum().item()\n",
    "            pred = output.argmax(dim =1 , keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(image), len(training_data.dataset),\n",
    "                100. * batch_idx / len(training_data), loss.item()))\n",
    "    print('-'*25)\n",
    "    print('training using Generator Data')\n",
    "    print('-'*25)\n",
    "    total_loss_gen = 0\n",
    "    correct_gen =0\n",
    "    if use_gen:\n",
    "        for i in range(batch_idx):\n",
    "            noise,idx,_,con = noise_sample(1,10,4,256,batch_size,device)\n",
    "            target = torch.LongTensor(idx).to(device)\n",
    "            image = netG(noise.squeeze((2,3)),target.view(-1,1),con)\n",
    "            label = torch.LongTensor(idx).to(device).squeeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            loss = nn.functional.cross_entropy(output, label)*0.9**epoch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                total_loss_gen += loss.sum().item()\n",
    "                pred = output.argmax(dim =1 , keepdim=True)\n",
    "                correct_gen += pred.eq(label.view_as(pred)).sum().item()\n",
    "            if i % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, i * len(image), len(training_data.dataset),\n",
    "                    100. * i / len(training_data), loss.item()))\n",
    "    if use_gen:\n",
    "        print('\\nTotal Training Set: \\n\\tAverage loss: {:.4f}\\n\\tAccuracy: {}/{} ({}%)'.format(\n",
    "            (total_loss+total_loss_gen)/len(training_data.dataset)/2,\n",
    "            correct+correct_gen,\n",
    "            len(training_data.dataset)*2,\n",
    "            100*(correct+correct_gen)/len(training_data.dataset)/2))\n",
    "    print('\\nTraining Set: \\n\\tAverage loss: {:.4f}\\n\\tAccuracy: {}/{} ({}%)'.format(\n",
    "        (total_loss)/len(training_data.dataset),\n",
    "        correct,\n",
    "        len(training_data.dataset),\n",
    "        100*(correct)/len(training_data.dataset)))\n",
    "    if use_gen:\n",
    "        print('\\nGenerator Set: \\n\\tAverage loss: {:.4f}\\n\\tAccuracy: {}/{} ({}%)'.format(\n",
    "            (total_loss_gen)/len(training_data.dataset),\n",
    "            correct_gen,\n",
    "            len(training_data.dataset),\n",
    "            100*correct_gen/len(training_data.dataset)))\n",
    "    \n",
    "    # wandb.watch(model)\n",
    "    # wandb.log({'Training Loss':total_loss/len(training_data.dataset),'Training Accuracy':correct/len(training_data.dataset)},commit = False)\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, test_data,scheduler, device):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the function to monitor a deep neural network's performance on validation data. Sends images and predictions to wandb\n",
    "    \n",
    "    Inputs:\n",
    "    {\n",
    "    model: The Neural network\n",
    "    test_data: test data with labels\n",
    "    device: Physical location of where data is stored (CPU or GPU)\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() #Set the model to \"evaluation\" mode and NOT compute gradients\n",
    "    total_loss = 0\n",
    "    correct = 0 \n",
    "    with torch.no_grad(): #Prevent pytorch from computing gradients\n",
    "        for image, label in test_data:\n",
    "            image, label = image.to(device), label.to(device) # place the input data into gpu ram or cpu ram\n",
    "            output = model(image)\n",
    "            total_loss = nn.functional.cross_entropy(output, label, reduction = 'sum').item()\n",
    "            pred = output.argmax(dim =1 , keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    total_loss /= len(test_data.dataset)\n",
    "    scheduler.step(total_loss)\n",
    "    print('Test set: \\n\\tAverage loss: {:.4f}\\n\\tAccuracy: {}/{} ({}%)\\n'.format(\n",
    "            total_loss,\n",
    "            correct, \n",
    "            len(test_data.dataset),\n",
    "            100. * correct /len(test_data.dataset)))\n",
    "    \n",
    "    \n",
    "    # wandb.log({'Validation Loss':total_loss,'Validation Accuracy':correct/len(test_data.dataset)},commit = False)\n",
    "            \n",
    "            \n",
    "    #######################################################################################################\n",
    "    # wandb_iter = iter(test_data)\n",
    "    # wandb_i,wandb_l = wandb_iter.next()\n",
    "    # with torch.no_grad():\n",
    "    #     wandb.log({'Predictions':[wandb.Image(wandb_i[i],caption = f\"Label: {int(wandb_l[i])}, Prediction: {int(torch.argmax(model(wandb_i[i].unsqueeze(0).to(device))))}\") for i in range(10)] },commit = True)\n",
    "    #######################################################################################################\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Determines if the model will be trained on gpu or cpu\n",
    "netG = Generator().to(device) #Initialize generator\n",
    "state_dict = torch.load(load_path)\n",
    "params = state_dict['params']\n",
    "netG.load_state_dict(state_dict['netG'])\n",
    "use_gen = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "Learning Rate:  6e-05 \n",
      "Batch Size:  32\n",
      "cuda\n",
      "Train Epoch: 0 [0/16000 (0%)]\tLoss: 2.299947\n",
      "Train Epoch: 0 [3200/16000 (20%)]\tLoss: 2.149292\n",
      "Train Epoch: 0 [6400/16000 (40%)]\tLoss: 1.908826\n",
      "Train Epoch: 0 [9600/16000 (60%)]\tLoss: 1.604508\n",
      "Train Epoch: 0 [12800/16000 (80%)]\tLoss: 1.454826\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 0 [0/16000 (0%)]\tLoss: 1.919851\n",
      "Train Epoch: 0 [3200/16000 (20%)]\tLoss: 1.674396\n",
      "Train Epoch: 0 [6400/16000 (40%)]\tLoss: 1.767294\n",
      "Train Epoch: 0 [9600/16000 (60%)]\tLoss: 1.313272\n",
      "Train Epoch: 0 [12800/16000 (80%)]\tLoss: 1.500361\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0520\n",
      "\tAccuracy: 13069/32000 (40.840625%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0147\n",
      "\tAccuracy: 2319/4000 (57.975%)\n",
      "\n",
      "Train Epoch: 1 [0/16000 (0%)]\tLoss: 1.317767\n",
      "Train Epoch: 1 [3200/16000 (20%)]\tLoss: 0.842776\n",
      "Train Epoch: 1 [6400/16000 (40%)]\tLoss: 0.798645\n",
      "Train Epoch: 1 [9600/16000 (60%)]\tLoss: 0.572954\n",
      "Train Epoch: 1 [12800/16000 (80%)]\tLoss: 0.651244\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 1 [0/16000 (0%)]\tLoss: 1.539023\n",
      "Train Epoch: 1 [3200/16000 (20%)]\tLoss: 1.074411\n",
      "Train Epoch: 1 [6400/16000 (40%)]\tLoss: 1.005913\n",
      "Train Epoch: 1 [9600/16000 (60%)]\tLoss: 1.194539\n",
      "Train Epoch: 1 [12800/16000 (80%)]\tLoss: 1.106136\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0286\n",
      "\tAccuracy: 21235/32000 (66.359375%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0091\n",
      "\tAccuracy: 2802/4000 (70.05%)\n",
      "\n",
      "Train Epoch: 2 [0/16000 (0%)]\tLoss: 0.749362\n",
      "Train Epoch: 2 [3200/16000 (20%)]\tLoss: 0.513341\n",
      "Train Epoch: 2 [6400/16000 (40%)]\tLoss: 0.326920\n",
      "Train Epoch: 2 [9600/16000 (60%)]\tLoss: 0.254628\n",
      "Train Epoch: 2 [12800/16000 (80%)]\tLoss: 0.364478\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 2 [0/16000 (0%)]\tLoss: 1.319559\n",
      "Train Epoch: 2 [3200/16000 (20%)]\tLoss: 1.044305\n",
      "Train Epoch: 2 [6400/16000 (40%)]\tLoss: 0.737102\n",
      "Train Epoch: 2 [9600/16000 (60%)]\tLoss: 1.766364\n",
      "Train Epoch: 2 [12800/16000 (80%)]\tLoss: 0.759559\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0209\n",
      "\tAccuracy: 23497/32000 (73.428125%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0063\n",
      "\tAccuracy: 2915/4000 (72.875%)\n",
      "\n",
      "Train Epoch: 3 [0/16000 (0%)]\tLoss: 0.748654\n",
      "Train Epoch: 3 [3200/16000 (20%)]\tLoss: 0.292870\n",
      "Train Epoch: 3 [6400/16000 (40%)]\tLoss: 0.376516\n",
      "Train Epoch: 3 [9600/16000 (60%)]\tLoss: 0.327758\n",
      "Train Epoch: 3 [12800/16000 (80%)]\tLoss: 0.297082\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 3 [0/16000 (0%)]\tLoss: 1.393563\n",
      "Train Epoch: 3 [3200/16000 (20%)]\tLoss: 0.597497\n",
      "Train Epoch: 3 [6400/16000 (40%)]\tLoss: 0.986147\n",
      "Train Epoch: 3 [9600/16000 (60%)]\tLoss: 0.726916\n",
      "Train Epoch: 3 [12800/16000 (80%)]\tLoss: 0.756196\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0178\n",
      "\tAccuracy: 24202/32000 (75.63125%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0047\n",
      "\tAccuracy: 2780/4000 (69.5%)\n",
      "\n",
      "Train Epoch: 4 [0/16000 (0%)]\tLoss: 0.604509\n",
      "Train Epoch: 4 [3200/16000 (20%)]\tLoss: 0.374358\n",
      "Train Epoch: 4 [6400/16000 (40%)]\tLoss: 0.165484\n",
      "Train Epoch: 4 [9600/16000 (60%)]\tLoss: 0.408033\n",
      "Train Epoch: 4 [12800/16000 (80%)]\tLoss: 0.200130\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 4 [0/16000 (0%)]\tLoss: 1.059341\n",
      "Train Epoch: 4 [3200/16000 (20%)]\tLoss: 0.546221\n",
      "Train Epoch: 4 [6400/16000 (40%)]\tLoss: 0.642613\n",
      "Train Epoch: 4 [9600/16000 (60%)]\tLoss: 0.552932\n",
      "Train Epoch: 4 [12800/16000 (80%)]\tLoss: 0.681645\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0148\n",
      "\tAccuracy: 24811/32000 (77.534375%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0052\n",
      "\tAccuracy: 2885/4000 (72.125%)\n",
      "\n",
      "Train Epoch: 5 [0/16000 (0%)]\tLoss: 0.738615\n",
      "Train Epoch: 5 [3200/16000 (20%)]\tLoss: 0.201461\n",
      "Train Epoch: 5 [6400/16000 (40%)]\tLoss: 0.142742\n",
      "Train Epoch: 5 [9600/16000 (60%)]\tLoss: 0.148006\n",
      "Train Epoch: 5 [12800/16000 (80%)]\tLoss: 0.122914\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 5 [0/16000 (0%)]\tLoss: 1.220170\n",
      "Train Epoch: 5 [3200/16000 (20%)]\tLoss: 0.559248\n",
      "Train Epoch: 5 [6400/16000 (40%)]\tLoss: 0.399360\n"
     ]
    }
   ],
   "source": [
    "#################### WANDB Setup ###################\n",
    "# wandb.init(project = 'ECE6254'\n",
    "#         ,config = {'learning_rate':0.01, 'batch_size':64}\n",
    "#             )\n",
    "# config = wandb.config\n",
    "\n",
    "# batch_size_train = config.batch_size\n",
    "# batch_size_test = 1000\n",
    "# learning_rate = config.learning_rate\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "##### Setup #####\n",
    "learning_rate = 6e-5\n",
    "num_epochs = 10 # Number of times to look over the data.\n",
    "model_ft = Net() # Initialize the model\n",
    "model_ft = model_ft.to(device) # Send the model to Ram or GPU Ram\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate, weight_decay = 7e-5) # Initialize optimizer\n",
    "exp_lr_scheduler = lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft,patience=3,threshold=0.01) #initialize scheduler. Every (1) epoch, reduce the learning rate by a factor of 0.7\n",
    "#################\n",
    "\n",
    "print(len(train_batch.dataset))\n",
    "print(\"Learning Rate: \",learning_rate,\"\\nBatch Size: \", batch_size)\n",
    "print(device)\n",
    "\n",
    "##### Main Loop for Training ######\n",
    "for epoch in range(num_epochs):\n",
    "            train_model(model_ft,\n",
    "                        train_batch,\n",
    "                        device, \n",
    "                        optimizer_ft, \n",
    "                        epoch,netG)\n",
    "            validate_model(model_ft,\n",
    "                        val_batch,\n",
    "                        exp_lr_scheduler,\n",
    "                        device) \n",
    "###################################                         \n",
    "\n",
    "#################################################\n",
    "# wandb.finish()\n",
    "#################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(1,4):\n",
    "    val = iter(val_batch)\n",
    "    img_val,_ = next(val)\n",
    "    input_val = img_val.to(device = device)\n",
    "    img_val = img_val.squeeze()\n",
    "    model_ft.eval()\n",
    "    output = model_ft(input_val)\n",
    "    # ae_output = output.squeeze().cpu().detach().numpy()\n",
    "    # print(output)\n",
    "    # k = model_ft.forward_encoder(input)\n",
    "    # print(k.shape)\n",
    "\n",
    "    print(output)\n",
    "    plt.imshow(img_val[0])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "945a0cfe9be83e1d07d52518592eeae449cb339e6c04ecdfcf1a86b9d6be3856"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
