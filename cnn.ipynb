{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "load_path = 'checkpoint/model_final_HGD'\n",
    "\n",
    "from models.cnnmodel import Net\n",
    "from models.hgd_model2 import Generator\n",
    "from HandGestureDataset import HandGestureDataSet as HGD\n",
    "from utils import HGDThreshold as HGDT\n",
    "from utils import noise_sample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Krishanu/Desktop/Education/Programcodes/Python/DeepLearning/Project/leapGestRecog/\"\n",
    "training = HGD(root = path, train = True,\n",
    "    transform = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "              \n",
    "                # T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Resize((240,240)),\n",
    "                # T.CenterCrop((190,180)),\n",
    "                T.RandomCrop((180,180)),\n",
    "                T.Resize((64,64)),\n",
    "                T.RandomRotation(20),\n",
    "                T.ToTensor(), \n",
    "                HGDT(55.0/256.0),\n",
    "                # T.Normalize(100/256.0,1),\n",
    "                # T.RandomAdjustSharpness(sharpness_factor = 4,p=0.5),\n",
    "                # T.RandomAutocontrast(p=1),\n",
    "\n",
    "                ])\n",
    "            )\n",
    "validation = HGD(root = path, train= False,\n",
    "    transform = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "              \n",
    "                # T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Resize((240,240)),\n",
    "                # T.CenterCrop((190,180)),\n",
    "                T.RandomCrop((180,180)),\n",
    "                T.Resize((64,64)),\n",
    "                T.RandomRotation(20),\n",
    "                T.ToTensor(), \n",
    "                HGDT(55.0/256.0),\n",
    "                # T.Normalize(100/256.0,1),\n",
    "                # T.RandomAdjustSharpness(sharpness_factor = 4,p=0.5),\n",
    "                # T.RandomAutocontrast(p=1),\n",
    "\n",
    "                ])\n",
    "            )\n",
    "\n",
    "batch_size = 32\n",
    "train_batch = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers= 8)\n",
    "val_batch = DataLoader(validation, batch_size=batch_size, shuffle=True, num_workers= 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = str(Path.home())+'/Documents/data/leapGestRecog/' #\n",
    "# data = HGD(root = path, train = True,\n",
    "#             transform = T.Compose([\n",
    "#                 T.ToPILImage(),\n",
    "#                 # T.CenterCrop(240),\n",
    "#                 T.Resize((64,64)),\n",
    "#                 T.ToTensor()\n",
    "#                 ]\n",
    "#             ))\n",
    "# # print(len(data))\n",
    "# training, validation = torch.utils.data.random_split(data, [16000,4000])\n",
    "# batch_size = 32\n",
    "# train_batch = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers= 8)\n",
    "# val_batch = DataLoader(validation, batch_size=batch_size, shuffle=True, num_workers= 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data, device, optimizer, epoch, netG):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the main function for training a deep neural network.\n",
    "    Inputs:\n",
    "    {\n",
    "    model: The Neural network\n",
    "    training_data: Training data with labels\n",
    "    device: Physical location of where data is stored (\"CPU\" or \"GPU\")\n",
    "    optimizer: Optimizer Function e.g. torch.optim.adam\n",
    "    scheduler: The type of scheduling for modifying the learning rate\n",
    "    num_epochs: number of iterations to train on the data\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    model.train() #Set the model to \"training\" mode and compute gradients\n",
    "    for batch_idx, (image, label) in enumerate(training_data):\n",
    "        image, label = image.to(device), label.to(device) # place the input data into gpu ram or cpu ram\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = nn.functional.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            total_loss += loss.sum().item()\n",
    "            pred = output.argmax(dim =1 , keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(image), len(training_data.dataset),\n",
    "                100. * batch_idx / len(training_data), loss.item()))\n",
    "    print('-'*25)\n",
    "    print('training using Generator Data')\n",
    "    print('-'*25)\n",
    "    for i in range(batch_idx):\n",
    "        noise,idx,_,con = noise_sample(1,10,4,256,batch_size,device)\n",
    "        target = torch.LongTensor(idx).to(device)\n",
    "        image = netG(noise.squeeze((2,3)),target.view(-1,1),con)\n",
    "        label = torch.LongTensor(idx).to(device).squeeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = nn.functional.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            total_loss += loss.sum().item()\n",
    "            pred = output.argmax(dim =1 , keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(image), len(training_data.dataset),\n",
    "                100. * i / len(training_data), loss.item()))\n",
    "    print('\\nTraining Set: \\n\\tAverage loss: {:.4f}\\n\\tAccuracy: {}/{} ({}%)'.format(\n",
    "        total_loss/len(training_data.dataset)/2,\n",
    "        correct,\n",
    "        len(training_data.dataset)*2,\n",
    "        100*correct/len(training_data.dataset)/2))\n",
    "    \n",
    "    # wandb.watch(model)\n",
    "    # wandb.log({'Training Loss':total_loss/len(training_data.dataset),'Training Accuracy':correct/len(training_data.dataset)},commit = False)\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, test_data,scheduler, device):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the function to monitor a deep neural network's performance on validation data. Sends images and predictions to wandb\n",
    "    \n",
    "    Inputs:\n",
    "    {\n",
    "    model: The Neural network\n",
    "    test_data: test data with labels\n",
    "    device: Physical location of where data is stored (CPU or GPU)\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() #Set the model to \"evaluation\" mode and NOT compute gradients\n",
    "    total_loss = 0\n",
    "    correct = 0 \n",
    "    with torch.no_grad(): #Prevent pytorch from computing gradients\n",
    "        for image, label in test_data:\n",
    "            image, label = image.to(device), label.to(device) # place the input data into gpu ram or cpu ram\n",
    "            output = model(image)\n",
    "            total_loss = nn.functional.cross_entropy(output, label, reduction = 'sum').item()\n",
    "            pred = output.argmax(dim =1 , keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    total_loss /= len(test_data.dataset)\n",
    "    scheduler.step(total_loss)\n",
    "    print('Test set: \\n\\tAverage loss: {:.4f}\\n\\tAccuracy: {}/{} ({}%)\\n'.format(\n",
    "            total_loss,\n",
    "            correct, \n",
    "            len(test_data.dataset),\n",
    "            100. * correct /len(test_data.dataset)))\n",
    "    \n",
    "    \n",
    "    # wandb.log({'Validation Loss':total_loss,'Validation Accuracy':correct/len(test_data.dataset)},commit = False)\n",
    "            \n",
    "            \n",
    "    #######################################################################################################\n",
    "    # wandb_iter = iter(test_data)\n",
    "    # wandb_i,wandb_l = wandb_iter.next()\n",
    "    # with torch.no_grad():\n",
    "    #     wandb.log({'Predictions':[wandb.Image(wandb_i[i],caption = f\"Label: {int(wandb_l[i])}, Prediction: {int(torch.argmax(model(wandb_i[i].unsqueeze(0).to(device))))}\") for i in range(10)] },commit = True)\n",
    "    #######################################################################################################\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Determines if the model will be trained on gpu or cpu\n",
    "netG = Generator().to(device) #Initialize generator\n",
    "state_dict = torch.load(load_path)\n",
    "params = state_dict['params']\n",
    "netG.load_state_dict(state_dict['netG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "Learning Rate:  6e-05 \n",
      "Batch Size:  32\n",
      "cuda\n",
      "Train Epoch: 0 [0/16000 (0%)]\tLoss: 2.298722\n",
      "Train Epoch: 0 [3200/16000 (20%)]\tLoss: 2.128924\n",
      "Train Epoch: 0 [6400/16000 (40%)]\tLoss: 1.753729\n",
      "Train Epoch: 0 [9600/16000 (60%)]\tLoss: 1.310762\n",
      "Train Epoch: 0 [12800/16000 (80%)]\tLoss: 1.689752\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 0 [0/16000 (0%)]\tLoss: 1.819194\n",
      "Train Epoch: 0 [3200/16000 (20%)]\tLoss: 1.728383\n",
      "Train Epoch: 0 [6400/16000 (40%)]\tLoss: 1.815762\n",
      "Train Epoch: 0 [9600/16000 (60%)]\tLoss: 1.380145\n",
      "Train Epoch: 0 [12800/16000 (80%)]\tLoss: 1.898782\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.1068\n",
      "\tAccuracy: 12437/32000 (38.865625%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0117\n",
      "\tAccuracy: 2105/8000 (26.3125%)\n",
      "\n",
      "Train Epoch: 1 [0/16000 (0%)]\tLoss: 1.304881\n",
      "Train Epoch: 1 [3200/16000 (20%)]\tLoss: 0.869342\n",
      "Train Epoch: 1 [6400/16000 (40%)]\tLoss: 0.954129\n",
      "Train Epoch: 1 [9600/16000 (60%)]\tLoss: 0.638284\n",
      "Train Epoch: 1 [12800/16000 (80%)]\tLoss: 0.417311\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 1 [0/16000 (0%)]\tLoss: 2.318980\n",
      "Train Epoch: 1 [3200/16000 (20%)]\tLoss: 1.036145\n",
      "Train Epoch: 1 [6400/16000 (40%)]\tLoss: 1.398409\n",
      "Train Epoch: 1 [9600/16000 (60%)]\tLoss: 1.463013\n",
      "Train Epoch: 1 [12800/16000 (80%)]\tLoss: 0.971955\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0656\n",
      "\tAccuracy: 20340/32000 (63.5625%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0077\n",
      "\tAccuracy: 2526/8000 (31.575%)\n",
      "\n",
      "Train Epoch: 2 [0/16000 (0%)]\tLoss: 1.229350\n",
      "Train Epoch: 2 [3200/16000 (20%)]\tLoss: 0.661598\n",
      "Train Epoch: 2 [6400/16000 (40%)]\tLoss: 0.577026\n",
      "Train Epoch: 2 [9600/16000 (60%)]\tLoss: 0.522806\n",
      "Train Epoch: 2 [12800/16000 (80%)]\tLoss: 0.522970\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 2 [0/16000 (0%)]\tLoss: 1.719287\n",
      "Train Epoch: 2 [3200/16000 (20%)]\tLoss: 1.595685\n",
      "Train Epoch: 2 [6400/16000 (40%)]\tLoss: 0.695404\n",
      "Train Epoch: 2 [9600/16000 (60%)]\tLoss: 1.326846\n",
      "Train Epoch: 2 [12800/16000 (80%)]\tLoss: 0.823487\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0510\n",
      "\tAccuracy: 22978/32000 (71.80625%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0063\n",
      "\tAccuracy: 2614/8000 (32.675%)\n",
      "\n",
      "Train Epoch: 3 [0/16000 (0%)]\tLoss: 0.570575\n",
      "Train Epoch: 3 [3200/16000 (20%)]\tLoss: 0.259574\n",
      "Train Epoch: 3 [6400/16000 (40%)]\tLoss: 0.278387\n",
      "Train Epoch: 3 [9600/16000 (60%)]\tLoss: 0.277177\n",
      "Train Epoch: 3 [12800/16000 (80%)]\tLoss: 0.335268\n",
      "-------------------------\n",
      "training using Generator Data\n",
      "-------------------------\n",
      "Train Epoch: 3 [0/16000 (0%)]\tLoss: 2.889659\n",
      "Train Epoch: 3 [3200/16000 (20%)]\tLoss: 1.052767\n",
      "Train Epoch: 3 [6400/16000 (40%)]\tLoss: 0.622781\n",
      "Train Epoch: 3 [9600/16000 (60%)]\tLoss: 0.895952\n",
      "Train Epoch: 3 [12800/16000 (80%)]\tLoss: 0.765821\n",
      "\n",
      "Training Set: \n",
      "\tAverage loss: 0.0449\n",
      "\tAccuracy: 24063/32000 (75.196875%)\n",
      "Test set: \n",
      "\tAverage loss: 0.0046\n",
      "\tAccuracy: 2784/8000 (34.8%)\n",
      "\n",
      "Train Epoch: 4 [0/16000 (0%)]\tLoss: 0.617073\n",
      "Train Epoch: 4 [3200/16000 (20%)]\tLoss: 0.335389\n",
      "Train Epoch: 4 [6400/16000 (40%)]\tLoss: 0.207906\n"
     ]
    }
   ],
   "source": [
    "#################### WANDB Setup ###################\n",
    "# wandb.init(project = 'ECE6254'\n",
    "#         ,config = {'learning_rate':0.01, 'batch_size':64}\n",
    "#             )\n",
    "# config = wandb.config\n",
    "\n",
    "# batch_size_train = config.batch_size\n",
    "# batch_size_test = 1000\n",
    "# learning_rate = config.learning_rate\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "##### Setup #####\n",
    "learning_rate = 6e-5\n",
    "num_epochs = 10 # Number of times to look over the data.\n",
    "model_ft = Net() # Initialize the model\n",
    "model_ft = model_ft.to(device) # Send the model to Ram or GPU Ram\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate, weight_decay = 7e-5) # Initialize optimizer\n",
    "exp_lr_scheduler = lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft,patience=3,threshold=0.01) #initialize scheduler. Every (1) epoch, reduce the learning rate by a factor of 0.7\n",
    "#################\n",
    "\n",
    "print(len(train_batch.dataset))\n",
    "print(\"Learning Rate: \",learning_rate,\"\\nBatch Size: \", batch_size)\n",
    "print(device)\n",
    "\n",
    "##### Main Loop for Training ######\n",
    "for epoch in range(num_epochs):\n",
    "            train_model(model_ft,\n",
    "                        train_batch,\n",
    "                        device, \n",
    "                        optimizer_ft, \n",
    "                        epoch,netG)\n",
    "            validate_model(model_ft,\n",
    "                        val_batch,\n",
    "                        exp_lr_scheduler,\n",
    "                        device) \n",
    "###################################                         \n",
    "\n",
    "#################################################\n",
    "# wandb.finish()\n",
    "#################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WANDB Setup ###################\n",
    "# wandb.init(project = 'ECE6254'\n",
    "#         ,config = {'learning_rate':0.01, 'batch_size':64}\n",
    "#             )\n",
    "# config = wandb.config\n",
    "\n",
    "# batch_size_train = config.batch_size\n",
    "# batch_size_test = 1000\n",
    "# learning_rate = config.learning_rate\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "##### Setup #####\n",
    "\n",
    "learning_rate = 9e-5\n",
    "num_epochs = 10 # Number of times to look over the data.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Determines if the model will be trained on gpu or cpu\n",
    "model_ft = Net()\n",
    "model_ft.load_state_dict(torch.load('autoencoder_dict'),strict = False) # Initialize the model\n",
    "# model_ft.encoder.requires_grad_(False)\n",
    "model_ft = model_ft.to(device) # Send the model to Ram or GPU Ram\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate, weight_decay = 7e-4) # Initialize optimizer\n",
    "exp_lr_scheduler = lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft,patience=3,threshold=0.01) #initialize scheduler. Every (1) epoch, reduce the learning rate by a factor of 0.7\n",
    "#################\n",
    "\n",
    "print(len(train_batch.dataset))\n",
    "print(\"Learning Rate: \",learning_rate,\"\\nBatch Size: \", batch_size)\n",
    "\n",
    "\n",
    "##### Main Loop for Training ######\n",
    "for epoch in range(num_epochs):\n",
    "            train_model(model_ft,\n",
    "                        train_batch,\n",
    "                        device, \n",
    "                        optimizer_ft, \n",
    "                        epoch)\n",
    "            validate_model(model_ft,\n",
    "                        val_batch,\n",
    "                        exp_lr_scheduler,\n",
    "                        device) \n",
    "###################################                         \n",
    "\n",
    "#################################################\n",
    "# wandb.finish()\n",
    "#################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(1,4):\n",
    "    val = iter(val_batch)\n",
    "    img_val,_ = next(val)\n",
    "    input_val = img_val.to(device = device)\n",
    "    img_val = img_val.squeeze()\n",
    "    model_ft.eval()\n",
    "    output = model_ft(input_val)\n",
    "    ae_output = output.squeeze().cpu().detach().numpy()\n",
    "    # print(output)\n",
    "    # k = model_ft.forward_encoder(input)\n",
    "    # print(k.shape)\n",
    "\n",
    "\n",
    "    plt.imshow(img_val[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = torch.load('autoencoder_dict')\n",
    "print(hello['decoder.7.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "945a0cfe9be83e1d07d52518592eeae449cb339e6c04ecdfcf1a86b9d6be3856"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
